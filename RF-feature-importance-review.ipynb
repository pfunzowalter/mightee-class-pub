{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245a2b77-3fae-4ed6-bb11-e63e3910a57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "import matplotlib.pyplot as plt\n",
    "import os \n",
    "import os.path as osp\n",
    "import itertools\n",
    "import astropy.io.fits as fits\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score \n",
    "# from sklearn.model_selection import Kfold\n",
    "from sklearn.metrics import accuracy_score as accuracy\n",
    "from sklearn.metrics import f1_score as f1\n",
    "from sklearn.metrics import recall_score as recall\n",
    "from sklearn.metrics import precision_score as precision\n",
    "from sklearn.metrics import classification_report, make_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b00579e-1808-45c5-bf9c-1eeff6f4aa0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the data from the machine\n",
    "file = osp.join(\"COSMOSXMATCH+classes_040422_withphotometry.fits\")\n",
    "fp = fits.open(file, memmap=True)\n",
    "head = fp[1].header\n",
    "\n",
    "data = fp[1].data\n",
    "fp.close\n",
    "\n",
    "# number of elements in the data\n",
    "N_all = len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927f5b04-23cc-478e-a8ae-ed037b07768c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA SAMPLING AND PREPROCESSING\n",
    "def data_processor( data, x_features, y_features, binary_classification = True ):\n",
    "    \"\"\" This function prepares the samples a subset of features for machine learning from a high dimensional data.\n",
    "        classification : type of classification, binary == SFG or AGNs, Other wise classification will be for AGNs\n",
    "        or SFG or noclass\n",
    "        data = high dimensional dataset (MIGHTEE data)\n",
    "        X-features = columns of interest contain data\n",
    "        Y-features = the output features \n",
    "        \n",
    "    \"\"\"\n",
    "    if binary_classification == True:\n",
    "        # extracting the x-features\n",
    "        x_sets = []\n",
    "        for i in x_features:\n",
    "            x = data[i]\n",
    "            x_sets.append(np.array(x))\n",
    "    \n",
    "        X = np.vstack((x_sets)).T\n",
    "    \n",
    "        #extracting the y-feature\n",
    "        y_sets = []\n",
    "        for l in y_features:\n",
    "            y = data[l]\n",
    "            y_sets.append(y)\n",
    "        \n",
    "        y = np.vstack((y_sets)).T\n",
    "    \n",
    "        # converting the features into the data frame\n",
    "        y_data = pd.DataFrame(y, columns = y_features)\n",
    "        X_data = pd.DataFrame(X, columns = x_features)\n",
    "\n",
    "        # joinin the two data sets into one dataframe\n",
    "        mightee_data = pd.concat([X_data, y_data], axis=1, sort=False)\n",
    "    \n",
    "        # Sampling the sources that are classified as midIRAGB = AGN and the sources that are classified as notmidIRAGN = SFG\n",
    "        AGN = mightee_data[mightee_data[y_features[0]] == True]\n",
    "        SFG = mightee_data[mightee_data[y_features[1]] == True]\n",
    "        probSFG = mightee_data[mightee_data[y_features[2]] == True]\n",
    "    \n",
    "        print('shape of the AGN: ', AGN.shape)\n",
    "        print('shape of the SFG: ', SFG.shape)\n",
    "        print('shape of the probSFG: ', probSFG.shape)\n",
    "        print('total sample: ', len(AGN) + len(SFG)+len(probSFG))\n",
    "    \n",
    "        # We now drop the not column\n",
    "        mightee_agn = AGN.drop([y_features[1], y_features[2]], axis = 1)\n",
    "        mightee_sfg = SFG.drop([y_features[1], y_features[2]], axis = 1)\n",
    "        mightee_probsfg = probSFG.drop([y_features[1], y_features[2]], axis = 1)\n",
    "\n",
    "        # We now replace True with the true label AGN or SFG for the corresponding source\n",
    "        mightee_agn1 = mightee_agn.replace(True, 'AGN', regex=True)\n",
    "        mightee_sfg1 = mightee_sfg.replace(False, 'SFG', regex=True)\n",
    "        mightee_probsfg1 = mightee_probsfg.replace(False, 'SFG', regex=True)\n",
    "    \n",
    "        # combining this data into one\n",
    "        complete_mightee = pd.concat([mightee_agn1, mightee_sfg1, mightee_probsfg1], sort=False)\n",
    "        complete_mightee1 = complete_mightee.replace(-np.inf, np.nan, regex=True) \n",
    "        print(\"DONE PROCESSING\")\n",
    "    \n",
    "        return complete_mightee1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    else:\n",
    "        x_sets = []\n",
    "        for i in x_features:\n",
    "            x = data[i]\n",
    "            x_sets.append(np.array(x))\n",
    "\n",
    "        X = np.vstack((x_sets)).T\n",
    "\n",
    "        #extracting the y-feature\n",
    "        y_sets = []\n",
    "        for l in y_features:\n",
    "            y = data[l]\n",
    "            y_sets.append(y)\n",
    "\n",
    "        y = np.vstack((y_sets)).T\n",
    "\n",
    "        # converting the features into the data frame\n",
    "        y_data = pd.DataFrame(y, columns = y_features)\n",
    "        X_data = pd.DataFrame(X, columns = x_features)\n",
    "\n",
    "        # joinin the two data sets into one dataframe\n",
    "        mightee_data = pd.concat([X_data, y_data], axis=1, sort=False)\n",
    "\n",
    "        # Sampling the sources that are classified as midIRAGB = AGN and the sources that are classified as notmidIRAGN = SFG\n",
    "        AGN = mightee_data[mightee_data[y_features[0]] == True]\n",
    "        SFG = mightee_data[mightee_data[y_features[1]] == True]\n",
    "        probSFG = mightee_data[mightee_data[y_features[2]] == True]\n",
    "        noclass = mightee_data[(mightee_data[y_features[0]] == False) & (mightee_data[y_features[1]] == False) & (mightee_data[y_features[2]] == False)]\n",
    "\n",
    "        print('shape of the AGN: ', AGN.shape)\n",
    "        print('shape of the SFG: ', SFG.shape)\n",
    "        print('shape of the probSFG: ', probSFG.shape)\n",
    "        print('shape of unclassified: ',noclass.shape)\n",
    "        print('total sample: ', len(AGN) + len(SFG) + len(probSFG) + len(noclass))\n",
    "\n",
    "        # We now drop the not column\n",
    "        mightee_agn = AGN.drop([y_features[1], y_features[2]], axis = 1)\n",
    "        mightee_sfg = SFG.drop([y_features[1], y_features[2]], axis = 1)\n",
    "        mightee_probsfg = probSFG.drop([y_features[1], y_features[2]], axis = 1)\n",
    "        mightee_noclass = noclass.drop([y_features[1], y_features[2]], axis = 1)\n",
    "\n",
    "        # We now replace True with the true label AGN or SFG for the corresponding source\n",
    "        mightee_agn1 = mightee_agn.replace(True, 'AGN', regex=True)\n",
    "        mightee_sfg1 = mightee_sfg.replace(False, 'SFG', regex=True)\n",
    "        mightee_probsfg1 = mightee_probsfg.replace(False, 'SFG', regex=True)\n",
    "        mightee_noclass1 = mightee_noclass.replace(False, 'noclass', regex=True)\n",
    "\n",
    "        # combining this data into one\n",
    "        complete_mightee = pd.concat([mightee_agn1, mightee_sfg1, mightee_probsfg1, mightee_noclass1], sort=False)\n",
    "        complete_mightee1 = complete_mightee.replace(-np.inf, np.nan, regex=True) \n",
    "        print(\"DONE PROCESSING\")\n",
    "    \n",
    "        return complete_mightee1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b8d384-4b89-422f-b923-f6d53d409307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important data columns for machine learning\n",
    "\n",
    "x_fea = ['COS_best_z_v5', 'SPLASH_1_FLUX', 'SPLASH_2_FLUX', 'SPLASH_3_FLUX', 'SPLASH_4_FLUX',\n",
    "            'L14','LIR_WHz','MASS_lephare', 'class_star','qir','flux_HSC-G','flux_HSC-R', 'flux_HSC-I','flux_HSC-Z',\n",
    "            'flux_J', 'flux_H', 'flux_Ks', 'flux_Y']\n",
    "y_fea = ['AGN', 'SFG', 'probSFG']\n",
    "\n",
    "\n",
    "trad_flux = data_processor(data, x_fea, y_fea)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30398b8-7dd0-4e7e-a471-8c41418bff06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We rename AGN column to labels\n",
    "\n",
    "trad_flux.rename(columns = {'AGN':'class_labels', 'MASS_lephare':'Mstar'}, inplace = True)\n",
    "\n",
    "trad_flux_clean1 = trad_flux.dropna()\n",
    "trad_flux_clean = trad_flux_clean1.drop(['class_star', 'qir'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d403a0a-703a-4a3e-9896-7fa13d2a86c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cols = ['COS_best_z_v5', 'SPLASH_1_FLUX', 'SPLASH_2_FLUX', 'SPLASH_3_FLUX', 'SPLASH_4_FLUX',\n",
    "            'L14','LIR_WHz','Mstar','flux_HSC-G','flux_HSC-R', 'flux_HSC-I','flux_HSC-Z',\n",
    "            'flux_J', 'flux_H', 'flux_Ks', 'flux_Y']\n",
    "\n",
    "x_cols_no_mstar = ['COS_best_z_v5', 'SPLASH_1_FLUX', 'SPLASH_2_FLUX', 'SPLASH_3_FLUX', 'SPLASH_4_FLUX',\n",
    "            'L14','LIR_WHz','flux_HSC-G','flux_HSC-R', 'flux_HSC-I','flux_HSC-Z',\n",
    "            'flux_J', 'flux_H', 'flux_Ks', 'flux_Y']\n",
    "x_trad = trad_flux_clean[x_cols]\n",
    "\n",
    "y_trad = trad_flux_clean['class_labels']\n",
    "\n",
    "# encoding target class\n",
    "y, clas = pd.factorize(y_trad)\n",
    "y_target = pd.DataFrame(y, columns = ['labels'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_trad, y_target, stratify = y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4606dbf9-dc6b-4d74-9ba7-daadd83d1ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The trad Fluxes\n",
    "\n",
    "S8_S45 = np.log10( trad_flux_clean1['SPLASH_4_FLUX'] / trad_flux_clean1['SPLASH_2_FLUX'])\n",
    "S58_S36 = np.log10( trad_flux_clean1['SPLASH_3_FLUX'] / trad_flux_clean1['SPLASH_1_FLUX'])\n",
    "S45_S36 = np.log10( trad_flux_clean1['SPLASH_2_FLUX'] / trad_flux_clean1['SPLASH_1_FLUX'])\n",
    "\n",
    "\n",
    "trad_flux_clean1['log(S8/S45)'] = S8_S45\n",
    "trad_flux_clean1['log(S58/S36)'] = S58_S36\n",
    "# trad_flux_clean1['log(S58/S36)'] = S8_S45\n",
    "# trad_flux_clean1['log(S8/S45)'] = S58_S36\n",
    "trad_flux_clean1['log(S45/S36)'] = S45_S36"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2180eaed-73f5-4fe2-a4d0-0cd18bbb843a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use color as features\n",
    "# magnitudes between two different filter bands.\n",
    "# features: [u-g, g-r, r-i, i-z]\n",
    "g_hsc = trad_flux_clean1['flux_HSC-G']\n",
    "r_hsc = trad_flux_clean1['flux_HSC-R']\n",
    "i_hsc = trad_flux_clean1['flux_HSC-I']\n",
    "z_hsc = trad_flux_clean1['flux_HSC-Z']   \n",
    "    \n",
    "# feaures\n",
    "g_r, r_i, i_z, g_i, g_z, r_z = np.array(np.log10(g_hsc / r_hsc)), np.array(np.log10(r_hsc /i_hsc)), np.array(np.log10(i_hsc /z_hsc)), np.array(np.log10(g_hsc / i_hsc)), np.array(np.log10(g_hsc / z_hsc)), np.array(np.log10(r_hsc / z_hsc))\n",
    "\n",
    "trad_flux_clean1['log(g/r)'], trad_flux_clean1['log(r/i)'], trad_flux_clean1['log(i/z)'], trad_flux_clean1['log(g/i)'], trad_flux_clean1['log(g/z)'], trad_flux_clean1['log(r/z)']  =  g_r, r_i, i_z, g_i, g_z, r_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba085df-867a-43ff-8e4c-200515495e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The NIR colours\n",
    "\n",
    "y_hsc = trad_flux_clean1['flux_Y']\n",
    "h_hsc = trad_flux_clean1['flux_H']\n",
    "k_hsc = trad_flux_clean1['flux_Ks']\n",
    "j_hsc = trad_flux_clean1['flux_J']\n",
    "    \n",
    "    \n",
    "# feaures\n",
    "y_j, j_h, h_k, y_h, y_k, j_k = np.array(np.log10(y_hsc / j_hsc)), np.array(np.log10(j_hsc /h_hsc)), np.array(np.log10(h_hsc / k_hsc)), np.array(np.log10(y_hsc / h_hsc)), np.array(np.log10(y_hsc / k_hsc)), np.array(np.log10(j_hsc / k_hsc))\n",
    "# hsc_columns = ['y/j', 'j/h', 'h/k']\n",
    "trad_flux_clean1['log(Y/J)'], trad_flux_clean1['log(J/H)'], trad_flux_clean1['$log(H/K_{s})$'], trad_flux_clean1['log(Y/H)'], trad_flux_clean1['$log(Y/K_{s})$'],trad_flux_clean1['$log(J/K_{s})$'] =  y_j, j_h, h_k,y_h, y_k, j_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af58fe39-abfa-43e0-b3ee-1a00ab2e34ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors_data = trad_flux_clean1.drop(x_cols_no_mstar, axis = 1)\n",
    "colors_data1 = colors_data.replace(-np.inf, np.nan, regex=True)\n",
    "colors = colors_data1.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37ad9a9-7706-4231-8639-fce14c2500ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5afb997-ab4a-4bae-b3f6-b99d2cde8dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Generate sample data (replace this with your actual dataset)\n",
    "# X = colors.drop('class_labels', axis=1)\n",
    "# y = colors['class_labels']\n",
    "# # encoding target class\n",
    "# y1, clas1 = pd.factorize(y)\n",
    "# target = pd.Series(y1)  # Changed to Series to avoid warning\n",
    "\n",
    "# feature_names = X.columns.tolist()  # Convert to list\n",
    "\n",
    "# def iterative_feature_importance(X, y, feature_names):\n",
    "#     \"\"\"\n",
    "#     Calculate feature importance iteratively by removing the most important feature at each step\n",
    "    \n",
    "#     Args:\n",
    "#     X: numpy array or pandas DataFrame of features\n",
    "#     y: numpy array or pandas Series of target variable\n",
    "#     feature_names: list of feature names\n",
    "    \n",
    "#     Returns:\n",
    "#     Dictionary containing importance values at each iteration\n",
    "#     \"\"\"\n",
    "#     X = pd.DataFrame(X, columns=feature_names)\n",
    "#     remaining_features = feature_names.copy()  # This is now a list\n",
    "#     importance_results = {}\n",
    "    \n",
    "#     iteration = 0\n",
    "#     while len(remaining_features) > 0:\n",
    "#         # Train Random Forest\n",
    "#         rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "#         rf.fit(X[remaining_features], y)\n",
    "        \n",
    "#         # Get feature importances\n",
    "#         importances = rf.feature_importances_\n",
    "#         importance_dict = dict(zip(remaining_features, importances))\n",
    "        \n",
    "#         # Store results\n",
    "#         importance_results[f'Iteration_{iteration}'] = {\n",
    "#             'features': remaining_features.copy(),\n",
    "#             'importances': importance_dict\n",
    "#         }\n",
    "        \n",
    "#         # Find and remove the most important feature (if more than one feature left)\n",
    "#         if len(remaining_features) > 1:\n",
    "#             most_important = max(importance_dict, key=importance_dict.get)\n",
    "#             remaining_features.remove(most_important)\n",
    "#             print(f\"Iteration {iteration}: Removed {most_important}\")\n",
    "#         else:\n",
    "#             print(f\"Final iteration: Only {remaining_features[0]} remains\")\n",
    "#             break\n",
    "            \n",
    "#         iteration += 1\n",
    "    \n",
    "#     return importance_results\n",
    "\n",
    "# # Run the analysis\n",
    "# results = iterative_feature_importance(X, target, feature_names)\n",
    "\n",
    "# # Plot the results\n",
    "# plt.figure(figsize=(12, 8))\n",
    "\n",
    "# for i, (key, value) in enumerate(results.items()):\n",
    "#     features = value['features']\n",
    "#     importances = value['importances']\n",
    "    \n",
    "#     # Sort features by importance\n",
    "#     sorted_features = sorted(importances.items(), key=lambda x: x[1], reverse=True)\n",
    "#     features_sorted = [f[0] for f in sorted_features]\n",
    "#     importance_values = [f[1] for f in sorted_features]\n",
    "    \n",
    "#     plt.subplot(len(results), 1, i+1)\n",
    "#     plt.barh(features_sorted, importance_values)\n",
    "#     plt.title(f'{key} - {len(features)} features remaining')\n",
    "#     plt.xlabel('Importance Score')\n",
    "#     plt.xlim(0, 1)\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# # Print results in a format similar to your screenshot\n",
    "# print(\"\\n# Importance\")\n",
    "# for i, (key, value) in enumerate(results.items()):\n",
    "#     features = value['features']\n",
    "#     importances = value['importances']\n",
    "    \n",
    "#     if i == 0:\n",
    "#         print(f\"- **Input**: {importances}\")\n",
    "#     else:\n",
    "#         print(f\"- **Output after removing top feature**: {importances}\")\n",
    "    \n",
    "#     if i < len(results) - 1:\n",
    "#         print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16c511a-4a30-4da5-b9de-167972d51b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sample data\n",
    "X = colors.drop('class_labels', axis=1)\n",
    "y = colors['class_labels']\n",
    "# encoding target class\n",
    "y1, clas1 = pd.factorize(y)\n",
    "target = pd.Series(y1)  # Using Series to avoid warning\n",
    "\n",
    "feature_names = X.columns.tolist()  # Convert to list\n",
    "\n",
    "def iterative_feature_importance(X, y, feature_names):\n",
    "    \"\"\"\n",
    "    Calculate feature importance iteratively by removing the most important feature at each step\n",
    "    \"\"\"\n",
    "    X = pd.DataFrame(X, columns=feature_names)\n",
    "    remaining_features = feature_names.copy()\n",
    "    importance_results = {}\n",
    "    \n",
    "    iteration = 0\n",
    "    while len(remaining_features) > 0:\n",
    "        # Train Random Forest\n",
    "        rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "        rf.fit(X[remaining_features], y)\n",
    "        \n",
    "        # Get feature importances\n",
    "        importances = rf.feature_importances_\n",
    "        importance_dict = dict(zip(remaining_features, importances))\n",
    "        \n",
    "        # Store results\n",
    "        importance_results[f'Iteration {iteration}'] = {\n",
    "            'features': remaining_features.copy(),\n",
    "            'importances': importance_dict,\n",
    "            'n_features': len(remaining_features)\n",
    "        }\n",
    "        \n",
    "        # Find and remove the most important feature\n",
    "        if len(remaining_features) > 1:\n",
    "            most_important = max(importance_dict, key=importance_dict.get)\n",
    "            remaining_features.remove(most_important)\n",
    "            print(f\"Iteration {iteration}: Removed {most_important}\")\n",
    "        else:\n",
    "            print(f\"Final iteration: Only {remaining_features[0]} remains\")\n",
    "            break\n",
    "            \n",
    "        iteration += 1\n",
    "    \n",
    "    return importance_results\n",
    "\n",
    "# Run the analysis\n",
    "results = iterative_feature_importance(X, target, feature_names)\n",
    "\n",
    "# Plot the results in a grid with 3 columns\n",
    "n_iterations = len(results)\n",
    "n_cols = 3\n",
    "n_rows = (n_iterations + n_cols - 1) // n_cols  # Calculate needed rows\n",
    "\n",
    "plt.figure(figsize=(18, 5 * n_rows))  # Adjust figure size based on rows\n",
    "\n",
    "for i, (key, value) in enumerate(results.items()):\n",
    "    features = value['features']\n",
    "    importances = value['importances']\n",
    "    n_features = value['n_features']\n",
    "    \n",
    "    # Sort features by importance\n",
    "    sorted_features = sorted(importances.items(), key=lambda x: x[1], reverse=True)\n",
    "    features_sorted = [f[0] for f in sorted_features]\n",
    "    importance_values = [f[1] for f in sorted_features]\n",
    "    \n",
    "    # Create subplot (n_rows, n_cols, i+1)\n",
    "    plt.subplot(n_rows, n_cols, i+1)\n",
    "    bars = plt.barh(features_sorted, importance_values)\n",
    "    plt.title(f'{key}\\n({n_features} features remaining)')\n",
    "    plt.xlabel('Importance Score')\n",
    "    plt.xlim(0, max(importance_values) * 1.1)  # Add 10% padding\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar in bars:\n",
    "        width = bar.get_width()\n",
    "        plt.text(width, bar.get_y() + bar.get_height()/2,\n",
    "                f'{width:.3f}',\n",
    "                ha='left', va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('RF-importance.pdf')\n",
    "plt.show()\n",
    "\n",
    "# Print results summary\n",
    "print(\"\\n# Feature Importance Analysis\")\n",
    "for i, (key, value) in enumerate(results.items()):\n",
    "    features = value['features']\n",
    "    importances = value['importances']\n",
    "    \n",
    "    if i == 0:\n",
    "        print(f\"- **Initial feature importances**:\")\n",
    "    else:\n",
    "        print(f\"- **After iteration {i} (removed {most_important_removed})**:\")\n",
    "        most_important_removed = list(set(prev_features) - set(features))[0]\n",
    "    \n",
    "    for feat, imp in sorted(importances.items(), key=lambda x: x[1], reverse=True):\n",
    "        print(f\"  - {feat}: {imp:.4f}\")\n",
    "    \n",
    "    if i < len(results) - 1:\n",
    "        print(\"---\")\n",
    "    prev_features = features.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbd9fa2-364f-4897-9f4b-a68b79dae341",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ASTRO-PY3",
   "language": "python",
   "name": "astro-py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
