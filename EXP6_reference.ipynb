{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "241875aa-7c0e-41b4-8c75-86be181da2a5",
   "metadata": {},
   "source": [
    "## We now compare the results of each notebook in experiment 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "088cda08-14e0-4261-88e6-70e126f6b493",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sources.ml_f1 import*\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score as accuracy\n",
    "from sklearn.metrics import f1_score as f1\n",
    "from sklearn.metrics import recall_score as recall\n",
    "from sklearn.metrics import precision_score as precision\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.spatial import distance\n",
    "import pandas as pd\n",
    "\n",
    "# ML models  \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import xgboost\n",
    "from xgboost import plot_importance\n",
    "from sklearn.utils import resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e764f616-438e-4767-94aa-a3ba9459cc03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We call the test and train data saved from the processing notebook\n",
    "%store -r base_dict\n",
    "# mightee_data = pd.read_csv('raw_data.csv')\n",
    "mightee_data = pd.read_csv('normalised/scaled_raw_zs.csv')\n",
    "\n",
    "# The train data\n",
    "# X_data = pd.read_csv('X_train_table.csv')\n",
    "# y_data = pd.read_csv('y_train_table.csv')\n",
    "# # The Unseen test data\n",
    "# X_test = pd.read_csv('X_test_table.csv')\n",
    "# y_test = pd.read_csv('y_test_table.csv')\n",
    "# The train data\n",
    "X_data = pd.read_csv('normalised/X_train_zs.csv')\n",
    "y_data = pd.read_csv('normalised/y_train_zs.csv')\n",
    "\n",
    "# The Unseen test data\n",
    "X_test = pd.read_csv('normalised/X_test_zs.csv')\n",
    "y_test = pd.read_csv('normalised/y_test_zs.csv')\n",
    "\n",
    "y = y_data['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a337170f-81f6-4781-9b35-e6c656c0a92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine features and labels for sampling\n",
    "data = pd.concat([X_data, y], axis=1)\n",
    "\n",
    "# Separate classes\n",
    "class_counts = y.value_counts()\n",
    "majority_class = class_counts.idxmax()\n",
    "minority_class = class_counts.idxmin()\n",
    "\n",
    "df_majority = data[data['labels'] == majority_class]\n",
    "df_minority = data[data['labels'] == minority_class]\n",
    "\n",
    "# Downsample majority class\n",
    "df_majority_downsampled = resample(df_majority,\n",
    "                                   replace=False,    # sample without replacement\n",
    "                                   n_samples=len(df_minority),  # to match minority class\n",
    "                                   random_state=42)  # reproducible results\n",
    "\n",
    "# Combine minority class with downsampled majority class\n",
    "df_balanced = pd.concat([df_majority_downsampled, df_minority])\n",
    "\n",
    "# Split back into X and y\n",
    "X_balanced = df_balanced.drop('labels', axis=1)\n",
    "y_balanced = df_balanced['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82d91ed8-3f2c-4b16-912e-23ac709a2f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the dataframe as raw_data\n",
    "X_balanced.to_csv('normalised/X_train_bal.csv', index = False, header=True)\n",
    "y_balanced.to_csv('normalised/y_train_bal.csv', index = False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99b7c8c1-a04c-4c58-a73e-1fcb14153333",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(y_data['labels'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46495cb2-9692-48a7-ad1c-85b9ee42495e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3209"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_data['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97733712-28af-45ba-b831-ac22f105c785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of original SFGs 3209\n",
      "Length of original AGN 3209\n",
      "Length of balanced SFGs 2288\n",
      "Length of balanced AGN 2288\n"
     ]
    }
   ],
   "source": [
    "print(\"Length of original SFGs\", len(y_data['labels']==1))\n",
    "print(\"Length of original AGN\", len(y_data['labels']==0))\n",
    "print(\"Length of balanced SFGs\", len(df_balanced['labels']==1))\n",
    "print(\"Length of balanced AGN\", len(df_balanced['labels']==0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53300c4-e62c-4ffd-ae14-bf4c6c94b05b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c70d1a-cbf0-4541-99fe-88ec4e567a58",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293b8a19-9efd-467f-a887-cad2e681c751",
   "metadata": {},
   "source": [
    "#### RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "39813eae-45dd-4f78-8c15-f6abf7321d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "## random forest (RF)\n",
    "# The Random Hyper parameter Grid\n",
    "\n",
    "# number of trees in the forest\n",
    "n_estimators = [50, 100, 150]\n",
    "\n",
    "# Number of feature to consider at every split\n",
    "max_features = [2, 3]\n",
    "\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [5, 10]\n",
    "\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5]\n",
    "\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 3]\n",
    "\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "\n",
    "\n",
    "# Create the random grid\n",
    "rf_par = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth}\n",
    "               # 'min_samples_split': min_samples_split,\n",
    "               # 'min_samples_leaf': min_samples_leaf,\n",
    "               # 'bootstrap': bootstrap}\n",
    "\n",
    "\n",
    "\n",
    "rf_model= RandomForestClassifier(random_state=1)\n",
    "rf_par = dict(n_estimators=n_estimators)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61aa0d1-adde-4f00-a533-05ddc2a67c2f",
   "metadata": {},
   "source": [
    "#### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a83295e4-6ab0-426c-97dc-c9197b0af502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Super Vector Machines\n",
    "svm_model = SVC(kernel='linear')\n",
    "\n",
    "\n",
    "svm_par = {'gamma': np.linspace(0.0001, 10, 15)}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b621b5d1-66f1-4308-bdd1-19d36505e036",
   "metadata": {},
   "source": [
    "#### KNN and LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "05def368-a94b-4124-8863-1fc71e673218",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN model\n",
    "knn_model = KNeighborsClassifier()\n",
    "\n",
    "## KNN parameters\n",
    "knn_par = {'n_neighbors' : [5, 10, 15], 'p':[1, 2], 'weights' : ['uniform', 'distance'] }\n",
    "\n",
    "\n",
    "## logisitc regression (LR)\n",
    "solvers = ['newton-cg', 'lbfgs', 'liblinear']\n",
    "penalty = ['l2']\n",
    "c_values = [1000, 100, 10, 1.0, 0.1, 0.01, 0.001]\n",
    "\n",
    "lr_model = LogisticRegression()\n",
    "lr_par = dict(solver=solvers,penalty=penalty,C=c_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eca02d30-7514-4d70-999f-effa019e8daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up models and Parameters for a \"for loop\"  \n",
    "\n",
    "models = [[lr_model, 'lr'], [knn_model, 'knn'], [svm_model, 'svm']] #, [rf_model, 'rf']]\n",
    "\n",
    "parameters = [ lr_par, knn_par, svm_par] #, rf_par]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3bf1f2c0-4b72-408d-86a4-192a1862e09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_dicts = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f710928-e206-4f95-9dd4-c6c6a82fa867",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b5a613c5-a0b5-4407-bf3d-659c875f3a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [['qir'], \n",
    "            ['qir', 'class_star'],\n",
    "            ['qir', 'class_star', 'log(S8/S45)'],\n",
    "            ['qir', 'class_star', 'log(S8/S45)','log(S58/S36)'],\n",
    "            ['qir', 'class_star', 'log(S8/S45)','log(S58/S36)', 'Mstar'],\n",
    "            ['qir', 'class_star', 'log(S8/S45)','log(S58/S36)', 'Mstar', 'log(S45/S36)'],\n",
    "            ['qir', 'class_star', 'Mstar', 'log(S45/S36)']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5c419aed-7014-4036-b1b8-75c58bd87bc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr\n",
      "knn\n",
      "svm\n"
     ]
    }
   ],
   "source": [
    "splits = [0.2] #,0.4,0.6,0.8]\n",
    "\n",
    "# Loop through different ML models coupled with thier hyper paramter (use the same splits for all features)\n",
    "for m, par in zip(models, parameters):\n",
    "    for s in splits:\n",
    "        # X_train, X_vald, y_train, y_vald = train_test_split(X_balanced, y_balanced, test_size= s, random_state=1, stratify = y_balanced, shuffle = True)\n",
    "        X_train, X_vald, y_train, y_vald = train_test_split(X_data, y, test_size= s, random_state=1, stratify = y, shuffle = True)\n",
    "        key0 = str(m[1])\n",
    "        print(key0)\n",
    "        ml_dicts[key0] = {} # defining The main subkeys, which are the machine learning models\n",
    "        \n",
    "        i = 1\n",
    "        for f in features:\n",
    "            xtr =  X_train[f]\n",
    "            xva =  X_vald[f]\n",
    "            xte =  X_test[f]\n",
    "            \n",
    "            results = get_f1_ml (m[0], par, xtr, y_train, xva, y_vald, xte, y_test) # to get the f1 for the ml model\n",
    "\n",
    "            key = \"F\"+str((i)) # Create keys for the each feature set in order to reference results\n",
    "            ml_dicts[key0][key] = {}\n",
    "\n",
    "            ml_dicts[key0][key]['tot_f1_vald'] = results[0]\n",
    "            ml_dicts[key0][key]['tot_f1_test'] = results[1]\n",
    "            ml_dicts[key0][key]['jack_train'] = results[2]\n",
    "            ml_dicts[key0][key]['jack_vald'] = results[3]\n",
    "            ml_dicts[key0][key]['jack_test'] = results[4]\n",
    "            i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0ed226a0-ee02-47e1-8efa-b97f09eccfe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_all = []\n",
    "for m, d in zip (models, ml_dicts.keys()):\n",
    "    f1_arr_vald = []\n",
    "    f1_arr_test = []\n",
    "    sd_vald_arr = []\n",
    "    sd_arr = [] \n",
    "    \n",
    "    # print(ml_dicts[d])\n",
    "    for key in ml_dicts[d].keys():\n",
    "        f1_arr_vald.append(ml_dicts[d][key][ 'tot_f1_vald' ]) # append total valdation f1 score to an array\n",
    "        f1_arr_test.append(ml_dicts[d][key][ 'tot_f1_test' ]) # append total test f1 score to an array\n",
    "        \n",
    "        sd_train = jack_SD(np.zeros( len(ml_dicts[d][key][ 'jack_train' ]) ), ml_dicts[d][key][ 'jack_train' ])[0]\n",
    "        sd_vald = jack_SD(np.zeros( len(ml_dicts[d][key][ 'jack_vald' ]) ), ml_dicts[d][key][ 'jack_vald' ])[0]\n",
    "        sd_test = jack_SD(np.zeros( len(ml_dicts[d][key][ 'jack_test' ]) ), ml_dicts[d][key][ 'jack_test' ])[0]\n",
    "        \n",
    "        sd_v = np.sqrt( np.array((sd_train**2)) + np.array((sd_vald**2)))\n",
    "        sd = np.sqrt( np.array((sd_train**2)) + np.array((sd_test**2)))\n",
    "       \n",
    "        sd_vald_arr.append(sd_v)\n",
    "        sd_arr.append(sd)\n",
    "        # append the SD to the sd_arr\n",
    "    arr_all.append([ list(ml_dicts[d].keys()), f1_arr_vald, f1_arr_test, sd_vald_arr, sd_arr])    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fd4919a1-8980-407a-94da-96be465bf1a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sample 4279\n",
      "Length of train 2567\n",
      "Length of validation 642\n",
      "Length of test 1070\n",
      "Samples Fractions: [train(%): 59.991 vald(%): 15.004 test(%): 25.006]\n"
     ]
    }
   ],
   "source": [
    "total_sample = len(X_train) + len(X_vald) + len(X_test)\n",
    "\n",
    "# fractions\n",
    "tr = (len(X_train)/total_sample) * 100\n",
    "tv = (len(X_vald)/total_sample) * 100 \n",
    "t = (len(X_test)/total_sample) * 100\n",
    "\n",
    "print('Total sample', total_sample)\n",
    "print('Length of train', len(X_train))\n",
    "print('Length of validation', len(X_vald))\n",
    "print('Length of test', len(X_test))\n",
    "\n",
    "print('Samples Fractions: [train(%): {0:.3f} vald(%): {1:.3f} test(%): {2:.3f}]'.format(tr,tv,t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e43465-488e-4c40-81f2-15fd423a8ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['blue', 'green', 'orange', 'red']\n",
    "fig, axs = plt.subplots(2, figsize=(15, 9), sharex=True, sharey =True)\n",
    "\n",
    "count = 0\n",
    "n = 5\n",
    "\n",
    "space = []\n",
    "tickFeat = []\n",
    "\n",
    "for result, model, color in zip(arr_all, models, colors):\n",
    "    a = np.linspace(n*count, n*(1+count)-2,len(features)) # to get index on the x-axis\n",
    "    space.extend(a)\n",
    "    tickFeat.extend(result[0])\n",
    "    axs[0].errorbar( a, result[1], result[3], fmt='o', label =model[1], color = color)\n",
    "    axs[0].set_title( \"F1 Score for different features with for Machine learning models\", fontweight ='bold', fontsize =12)\n",
    "    axs[0].set_ylabel(\"F1 score(vald)\", fontweight ='bold', fontsize =12)\n",
    "    axs[0].set_ylim(.80, 1)\n",
    "    axs[0].legend(loc = 'lower left')\n",
    "    \n",
    "    axs[1].errorbar( a, result[2], result[4], fmt='o', label =model[1], color = color)\n",
    "    axs[1].set_xlabel(\"Features\", fontweight ='bold', fontsize =12)\n",
    "    axs[1].set_ylabel(\"F1 score(test)\", fontweight ='bold', fontsize =12)\n",
    "    axs[1].set_ylim(.80, 1)\n",
    "    axs[1].legend(loc = 'lower left')\n",
    "    \n",
    "    count += 1\n",
    "\n",
    "plt.xticks(space, tickFeat, rotation = 'vertical',  fontsize =12)\n",
    "plt.savefig('normalised/ml_photo')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac935577-c5c5-491c-b09c-c59feade6fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['blue', 'green', 'orange', 'red']\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "\n",
    "count = 0\n",
    "n = 5\n",
    "\n",
    "space = []\n",
    "tickFeat = []\n",
    "\n",
    "for result, model, color in zip(arr_all, models, colors):\n",
    "    a = np.linspace(n*count, n*(1+count)-2,len(features)) # to get index on the x-axis\n",
    "    space.extend(a)\n",
    "    tickFeat.extend(result[0])\n",
    "    plt.errorbar( a, result[1], result[3], fmt='o', label =model[1], color = color)\n",
    "    plt.title( \"F1 Score on validation dataset for different features with the SD\", fontweight ='bold', fontsize =12)\n",
    "    plt.ylabel(\"F1 score(validation data)\", fontweight = 'bold', fontsize =12)\n",
    "    plt.ylim(.80, 1)\n",
    "    plt.legend(loc = 'lower left')\n",
    "    \n",
    "    count += 1\n",
    "\n",
    "plt.xticks(space, tickFeat, rotation = 'vertical',  fontsize =12)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171ef3c1-0453-45be-8aca-0854e0d5e627",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['blue', 'green', 'orange', 'red']\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "\n",
    "count = 0\n",
    "n = 5\n",
    "\n",
    "space = []\n",
    "tickFeat = []\n",
    "\n",
    "for result, model, color in zip(arr_all, models, colors):\n",
    "    a = np.linspace(n*count, n*(1+count)-2,len(features)) # to get index on the x-axis\n",
    "    space.extend(a)\n",
    "    tickFeat.extend(result[0])\n",
    "    plt.errorbar( a, result[2], result[4], fmt='o', label =model[1], color = color)\n",
    "    plt.title( \"F1 Score on unseen test dataset for different features with the SD\", fontweight ='bold', fontsize =12)\n",
    "    plt.ylabel(\"F1 score(test data)\", fontweight = 'bold', fontsize =12)\n",
    "    plt.ylim(.80, 1)\n",
    "    plt.legend(loc = 'lower left')\n",
    "    \n",
    "    count += 1\n",
    "\n",
    "plt.xticks(space, tickFeat, rotation = 'vertical',  fontsize =12)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b6745c-c028-4646-975c-2cb666da318b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "colors = ['blue', 'green', 'orange', 'red']\n",
    "\n",
    "count = 0\n",
    "n = 5\n",
    "\n",
    "space = []\n",
    "tickFeat = []\n",
    "\n",
    "# Prepare a list to hold rows for the CSV\n",
    "csv_rows = []\n",
    "\n",
    "for result, model, color in zip(arr_all, models, colors):\n",
    "    a = np.linspace(n * count, n * (1 + count) - 2, len(features))  # same x-axis indexing\n",
    "    space.extend(a)\n",
    "    tickFeat.extend(result[0])\n",
    "\n",
    "    for xi, feature_name, f1_score, sd in zip(a, result[0], result[2], result[4]):\n",
    "        csv_rows.append([feature_name, xi, f1_score, sd, model[1]])\n",
    "\n",
    "    count += 1\n",
    "\n",
    "# Write to CSV\n",
    "with open('normalised/model_f1_scores_zscales.csv', mode='w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['Feature', 'X_Position', 'F1_Score', 'SD', 'Model'])\n",
    "    writer.writerows(csv_rows)\n",
    "\n",
    "print(\"CSV file 'model_f1_scores.csv' has been written.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c5b4f8-d407-45ab-bf8f-5d350358292b",
   "metadata": {},
   "source": [
    "# BASELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e0612d-9ef6-441f-a2e5-64eff46abf76",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Saving the Result\n",
    "def arrays_to_csv(array1, array2, column_name, output_file='output.csv'):\n",
    "    \"\"\"\n",
    "    Creates a DataFrame from two arrays and writes it to a CSV file.\n",
    "    \n",
    "    Parameters:\n",
    "    - array1: First array (will become first column)\n",
    "    - array2: Second array (will become second column)\n",
    "    - column_name: Tuple of column names (e.g., ('Column1', 'Column2'))\n",
    "    - output_file: Name of the output CSV file (default: 'output.csv')\n",
    "    \n",
    "    Returns:\n",
    "    - None (writes file to disk)\n",
    "    \"\"\"\n",
    "    # Create DataFrame from the arrays\n",
    "    df = pd.DataFrame({\n",
    "        column_name[0]: array1,\n",
    "        column_name[1]: array2\n",
    "    })\n",
    "    \n",
    "    # Write to CSV\n",
    "    df.to_csv(output_file, index=False)\n",
    "    print(f\"Successfully wrote data to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75111d4-cf1f-4a3c-b25f-a679ef60af0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# column_names = ['f1','err']\n",
    "\n",
    "# arrays_to_csv(result[2], result[4], column_names, output_file='normalised/f1_normalised.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ca9a5d-c0b8-4075-9d74-16f6f1d17b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"DOne\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbb62b3-d4c0-4545-999d-7fa6246bd4c0",
   "metadata": {},
   "source": [
    "# DONE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ASTRO-PY3",
   "language": "python",
   "name": "astro-py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
