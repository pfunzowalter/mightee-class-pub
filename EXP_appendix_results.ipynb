{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b392426-1fde-4355-aa91-789ec3a3c055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading necessary modlules\n",
    "\n",
    "import os \n",
    "import os.path as osp\n",
    "import itertools\n",
    "import astropy.io.fits as fits\n",
    "import astropy.units as u\n",
    "import astropy.constants as c\n",
    "from astropy.cosmology import FlatLambdaCDM\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as pl\n",
    "pl.rcParams['axes.labelsize'] = 16\n",
    "pl.rcParams['axes.titlesize'] = 16\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "# pl.style.use('seaborn')\n",
    "import timeit\n",
    "\n",
    "from astropy.stats import bayesian_blocks\n",
    "from astropy.wcs import WCS\n",
    "from astropy.coordinates import SkyCoord\n",
    "from astropy.table import Table\n",
    "\n",
    "from scipy.stats import binned_statistic,norm, bayes_mvs\n",
    "import sklearn\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score \n",
    "# from sklearn.model_selection import Kfold\n",
    "from sklearn.metrics import accuracy_score as accuracy\n",
    "from sklearn.metrics import f1_score as f1\n",
    "from sklearn.metrics import recall_score as recall\n",
    "from sklearn.metrics import precision_score as precision\n",
    "from sklearn.metrics import classification_report, make_scorer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from scipy.stats import gaussian_kde\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from astropy.wcs import WCS # my additional package\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from scipy.stats import gaussian_kde\n",
    "import xgboost\n",
    "from xgboost import plot_importance\n",
    "from pprint import pprint\n",
    "# pl.style.use('fivethirtyeight')\n",
    "pl.style.use('seaborn-ticks')\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79796288-8e6b-40f1-a7aa-5269faa40e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the data from the machine\n",
    "# file = osp.join(\"MIGHTEEXMATCH+allmulti+classes2.fits\")\n",
    "# file = osp.join(\"MIGHTEEXMATCH allmulti classes_081221.fits\")\n",
    "\n",
    "file = osp.join(\"COSMOSXMATCH+classes_040422_withphotometry.fits\")\n",
    "fp = fits.open(file, memmap=True)\n",
    "head = fp[1].header\n",
    "\n",
    "data = fp[1].data\n",
    "fp.close\n",
    "\n",
    "# number of elements in the data\n",
    "N_all = len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b26608e-fe88-41f4-87c9-c6fb171cc583",
   "metadata": {},
   "source": [
    "# IMPORTANT FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff3f8c5-c61b-44f9-abbd-00d2d731a2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA SAMPLING AND PREPROCESSING\n",
    "def data_processor( data, x_features, y_features, binary_classification = True ):\n",
    "    \"\"\" This function prepares the samples a subset of features for machine learning from a high dimensional data.\n",
    "        classification : type of classification, binary == SFG or AGNs, Other wise classification will be for AGNs\n",
    "        or SFG or noclass\n",
    "        data = high dimensional dataset (MIGHTEE data)\n",
    "        X-features = columns of interest contain data\n",
    "        Y-features = the output features \n",
    "        \n",
    "    \"\"\"\n",
    "    if binary_classification == True:\n",
    "        # extracting the x-features\n",
    "        x_sets = []\n",
    "        for i in x_features:\n",
    "            x = data[i]\n",
    "            x_sets.append(np.array(x))\n",
    "    \n",
    "        X = np.vstack((x_sets)).T\n",
    "    \n",
    "        #extracting the y-feature\n",
    "        y_sets = []\n",
    "        for l in y_features:\n",
    "            y = data[l]\n",
    "            y_sets.append(y)\n",
    "        \n",
    "        y = np.vstack((y_sets)).T\n",
    "    \n",
    "        # converting the features into the data frame\n",
    "        y_data = pd.DataFrame(y, columns = y_features)\n",
    "        X_data = pd.DataFrame(X, columns = x_features)\n",
    "\n",
    "        # joinin the two data sets into one dataframe\n",
    "        mightee_data = pd.concat([X_data, y_data], axis=1, sort=False)\n",
    "    \n",
    "        # Sampling the sources that are classified as midIRAGB = AGN and the sources that are classified as notmidIRAGN = SFG\n",
    "        AGN = mightee_data[mightee_data[y_features[0]] == True]\n",
    "        SFG = mightee_data[mightee_data[y_features[1]] == True]\n",
    "        probSFG = mightee_data[mightee_data[y_features[2]] == True]\n",
    "    \n",
    "        print('shape of the AGN: ', AGN.shape)\n",
    "        print('shape of the SFG: ', SFG.shape)\n",
    "        print('shape of the probSFG: ', probSFG.shape)\n",
    "        print('total sample: ', len(AGN) + len(SFG)+len(probSFG))\n",
    "    \n",
    "        # We now drop the not column\n",
    "        mightee_agn = AGN.drop([y_features[1], y_features[2]], axis = 1)\n",
    "        mightee_sfg = SFG.drop([y_features[1], y_features[2]], axis = 1)\n",
    "        mightee_probsfg = probSFG.drop([y_features[1], y_features[2]], axis = 1)\n",
    "\n",
    "        # We now replace True with the true label AGN or SFG for the corresponding source\n",
    "        mightee_agn1 = mightee_agn.replace(True, 'AGN', regex=True)\n",
    "        mightee_sfg1 = mightee_sfg.replace(False, 'SFG', regex=True)\n",
    "        mightee_probsfg1 = mightee_probsfg.replace(False, 'SFG', regex=True)\n",
    "    \n",
    "        # combining this data into one\n",
    "        complete_mightee = pd.concat([mightee_agn1, mightee_sfg1, mightee_probsfg1], sort=False)\n",
    "        complete_mightee1 = complete_mightee.replace(-np.inf, np.nan, regex=True) \n",
    "        print(\"DONE PROCESSING\")\n",
    "    \n",
    "        return complete_mightee1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    else:\n",
    "        x_sets = []\n",
    "        for i in x_features:\n",
    "            x = data[i]\n",
    "            x_sets.append(np.array(x))\n",
    "\n",
    "        X = np.vstack((x_sets)).T\n",
    "\n",
    "        #extracting the y-feature\n",
    "        y_sets = []\n",
    "        for l in y_features:\n",
    "            y = data[l]\n",
    "            y_sets.append(y)\n",
    "\n",
    "        y = np.vstack((y_sets)).T\n",
    "\n",
    "        # converting the features into the data frame\n",
    "        y_data = pd.DataFrame(y, columns = y_features)\n",
    "        X_data = pd.DataFrame(X, columns = x_features)\n",
    "\n",
    "        # joinin the two data sets into one dataframe\n",
    "        mightee_data = pd.concat([X_data, y_data], axis=1, sort=False)\n",
    "\n",
    "        # Sampling the sources that are classified as midIRAGB = AGN and the sources that are classified as notmidIRAGN = SFG\n",
    "        AGN = mightee_data[mightee_data[y_features[0]] == True]\n",
    "        SFG = mightee_data[mightee_data[y_features[1]] == True]\n",
    "        probSFG = mightee_data[mightee_data[y_features[2]] == True]\n",
    "        noclass = mightee_data[(mightee_data[y_features[0]] == False) & (mightee_data[y_features[1]] == False) & (mightee_data[y_features[2]] == False)]\n",
    "\n",
    "        print('shape of the AGN: ', AGN.shape)\n",
    "        print('shape of the SFG: ', SFG.shape)\n",
    "        print('shape of the probSFG: ', probSFG.shape)\n",
    "        print('shape of unclassified: ',noclass.shape)\n",
    "        print('total sample: ', len(AGN) + len(SFG) + len(probSFG) + len(noclass))\n",
    "\n",
    "        # We now drop the not column\n",
    "        mightee_agn = AGN.drop([y_features[1], y_features[2]], axis = 1)\n",
    "        mightee_sfg = SFG.drop([y_features[1], y_features[2]], axis = 1)\n",
    "        mightee_probsfg = probSFG.drop([y_features[1], y_features[2]], axis = 1)\n",
    "        mightee_noclass = noclass.drop([y_features[1], y_features[2]], axis = 1)\n",
    "\n",
    "        # We now replace True with the true label AGN or SFG for the corresponding source\n",
    "        mightee_agn1 = mightee_agn.replace(True, 'AGN', regex=True)\n",
    "        mightee_sfg1 = mightee_sfg.replace(False, 'SFG', regex=True)\n",
    "        mightee_probsfg1 = mightee_probsfg.replace(False, 'SFG', regex=True)\n",
    "        mightee_noclass1 = mightee_noclass.replace(False, 'noclass', regex=True)\n",
    "\n",
    "        # combining this data into one\n",
    "        complete_mightee = pd.concat([mightee_agn1, mightee_sfg1, mightee_probsfg1, mightee_noclass1], sort=False)\n",
    "        complete_mightee1 = complete_mightee.replace(-np.inf, np.nan, regex=True) \n",
    "        print(\"DONE PROCESSING\")\n",
    "    \n",
    "        return complete_mightee1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5135e47c-2eef-448c-9dfa-7f08c1711650",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLASSIFIER AND METRICS ALGORITHMS\n",
    "\n",
    "def classifier(model, X_features, y_features):\n",
    "    #     getting the indices\n",
    "    indices = np.arange(X_features.shape[0])\n",
    "    \n",
    "    X_dummies = pd.get_dummies(X_features)\n",
    "    \n",
    "    y_target, clas = pd.factorize(y_features) #getting the class 0 = agn, 1 =notagn, 2 = no class\n",
    "    \n",
    "    #     we split the sample into the testing and training\n",
    "    x_train, x_test, y_train, y_test, i_train, i_test = train_test_split(X_dummies, y_target, indices, test_size=0.20)\n",
    "    \n",
    "    # Source Classification\n",
    "    start_time = timeit.default_timer()\n",
    "\n",
    "    model.fit(x_train, y_train)  \n",
    "\n",
    "    y_xgb = model.predict(x_test)\n",
    "\n",
    "    elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "    proba = xgb_model.predict_proba(x_test)\n",
    "\n",
    "\n",
    "    acu = accuracy(y_test, y_xgb)\n",
    "    \n",
    "    print('Elapsed time for XGB: {} seconds'.format(elapsed))\n",
    "    print(len(y_xgb))\n",
    "    print('Accuracy for XGB is: {}'.format(acu))\n",
    "    print(metrics.classification_report(y_test, y_xgb, target_names=clas, digits=4))\n",
    "    \n",
    "def confusion_matrix(cm, classes, \n",
    "                        name = '',\n",
    "                        normalize=False,\n",
    "                        title='Confusion matrix',\n",
    "                        cmap=pl.cm.Greens):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    pl.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    pl.title(title)\n",
    "    pl.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    pl.xticks(tick_marks, classes, rotation=45)\n",
    "    pl.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "    \n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        pl.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    pl.tight_layout()\n",
    "    pl.ylabel('True label')\n",
    "    pl.xlabel('Predicted label')\n",
    "    pl.savefig(name)\n",
    "    pl.show()# Feature importance for the experiment\n",
    "    \n",
    "# Feature importance for the experiment\n",
    "def feature_importance(data):\n",
    "    importances = pd.DataFrame({\n",
    "        'Feature': data.drop('AGN', axis=1).columns,\n",
    "        'Importance': xgb_model.feature_importances_\n",
    "    })\n",
    "    importances = importances.sort_values(by='Importance', ascending=False)\n",
    "    importances = importances.set_index('Feature')\n",
    "#     print(importances)\n",
    "    \n",
    "    \n",
    "    pl.figure(figsize = (16, 10))\n",
    "\n",
    "    importances.plot.bar()\n",
    "    pl.show()\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c562737e-6de0-404c-8861-731138546a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The train_VS_score manual cross validation\n",
    "def train_vs_score_cv( model, X, y, train_size, binary = True):\n",
    "    \n",
    "    if binary == True:\n",
    "        #         Scores per class\n",
    "        agn_f1, sfg_f1 = [], []\n",
    "        agn_rec, sfg_rec = [], []\n",
    "        agn_pre, sfg_pre = [], []\n",
    "        rand_stat = np.arange(10, 400, 20)\n",
    "        for i in range(len(train_size)):\n",
    "            # define the test size\n",
    "            test_size = 1 - train_size[i]\n",
    "            # ramdomly spliting the data n times for each test-size\n",
    "            agn_f1_random, sfg_f1_random = [], []\n",
    "            agn_rec_random, sfg_rec_random = [], []\n",
    "            agn_pre_random, sfg_pre_random = [], []\n",
    "            n  = 0\n",
    "            while n < 20:\n",
    "            #     we split the sample into the testing and training\n",
    "                X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = test_size, random_state = rand_stat[n])\n",
    "\n",
    "            # Source Classification\n",
    "                model.fit(X_train, np.ravel(y_train))  \n",
    "\n",
    "                y_xgb = model.predict(X_test)\n",
    "                \n",
    "                agn_f1_sco1  = f1(y_test, y_xgb, average = 'weighted', labels = [0])\n",
    "                sfg_f1_sco1  = f1(y_test, y_xgb, average = 'weighted', labels = [1])\n",
    "                \n",
    "                \n",
    "                agn_rec1 = recall(y_test, y_xgb, average = 'weighted', labels = [0])\n",
    "                sfg_rec1 = recall(y_test, y_xgb, average = 'weighted', labels = [1])\n",
    "\n",
    "                agn_pre1 = precision(y_test, y_xgb, average = 'weighted', labels = [0])\n",
    "                sfg_pre1 = precision(y_test, y_xgb, average = 'weighted', labels = [1])\n",
    "        \n",
    "\n",
    "                agn_f1_random.append(agn_f1_sco1)\n",
    "                sfg_f1_random.append(sfg_f1_sco1)\n",
    "                \n",
    "                agn_rec_random.append(agn_rec1)\n",
    "                sfg_rec_random.append(sfg_rec1)\n",
    "                \n",
    "                agn_pre_random.append(agn_pre1)\n",
    "                sfg_pre_random.append(sfg_pre1)\n",
    "                n = n + 1\n",
    "\n",
    "            agn_f1.append(np.mean(agn_f1_random))\n",
    "            sfg_f1.append(np.mean(sfg_f1_random))\n",
    "            \n",
    "            agn_rec.append(np.mean(agn_rec_random))\n",
    "            sfg_rec.append(np.mean(sfg_rec_random))\n",
    "            \n",
    "            agn_pre.append(np.mean(agn_pre_random))\n",
    "            sfg_pre.append(np.mean(sfg_pre_random))\n",
    "\n",
    "        # plot style\n",
    "        import seaborn as sns\n",
    "        sns.set_style(\"whitegrid\", {\"axes.facecolor\": \".98\"})\n",
    "        \n",
    "        # Three subplots sharing both x/y axes\n",
    "        f, (ax1, ax2, ax3) = pl.subplots(3, figsize=(13, 10), sharex=True) #, sharey=True)\n",
    "        #F1\n",
    "        ax3.plot(train_size, agn_f1,'--', label = 'AGN', c = 'r', linewidth = 3, alpha = 1)\n",
    "        ax3.plot(train_size, sfg_f1, '-.', label = 'SFG', c = 'g', linewidth = 3, alpha = 1)\n",
    "        ax3.set_ylabel('F1', fontweight ='bold', fontsize =15)\n",
    "        ax3.set_xlabel('size of train data', fontweight ='bold', fontsize =15)\n",
    "        ax3.legend(loc = 'lower right')\n",
    "        \n",
    "        #precision\n",
    "        ax1.plot(train_size, agn_pre, '--', c = 'r', label = 'AGN', linewidth = 3, alpha = 1)\n",
    "        ax1.plot(train_size, sfg_pre, '-.', c = 'g', label = 'SFG', linewidth = 3, alpha = 1)\n",
    "        # ax1.set_title('RF model trained only with flux density')\n",
    "        ax1.set_ylabel('precision', fontweight ='bold', fontsize =15)\n",
    "        ax1.legend(loc = 'lower right')\n",
    "        \n",
    "        #Recall\n",
    "        ax2.plot(train_size, agn_rec, '--', c = 'r', label = 'AGN', linewidth = 3, alpha = 1)\n",
    "        ax2.plot(train_size, sfg_rec, '-.', c = 'g', label = 'SFG', linewidth = 3, alpha = 1)\n",
    "        ax2.set_ylabel('recall', fontweight ='bold', fontsize =15)\n",
    "        ax2.legend(loc = 'lower right')\n",
    "\n",
    "        f.subplots_adjust(hspace=0)\n",
    "        pl.setp([a.get_xticklabels() for a in f.axes[:-1]], visible=False)\n",
    "        pl.savefig('color_ml_results')\n",
    "        pl.show()\n",
    "\n",
    "        #         converting scores to a dataframe\n",
    "        agn_metrics = np.vstack([np.array(train_size),\n",
    "                                 np.array(agn_f1),\n",
    "                                 np.array(agn_pre),\n",
    "                                 np.array(agn_rec)]).T\n",
    "    \n",
    "    \n",
    "        sfg_metrics = np.vstack([np.array(train_size),\n",
    "                                 np.array(sfg_f1),\n",
    "                                 np.array(sfg_pre),\n",
    "                                 np.array(sfg_rec)]).T\n",
    "        \n",
    "        agn_score_data = pd.DataFrame(agn_metrics, columns = ['train_size', 'f1', 'precision', 'recall'])\n",
    "        sfg_score_data = pd.DataFrame(sfg_metrics, columns = ['train_size', 'f1', 'precision', 'recall'])\n",
    "        \n",
    "        return agn_score_data, sfg_score_data\n",
    "        \n",
    "    else:\n",
    "        print('Error; only works for a binary classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c84dc38-0a1b-4313-99c9-bccbf6917280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drawing a table for results\n",
    "from tabulate import tabulate\n",
    "  \n",
    "def mytable(agn_cat, sfg_cat, features):\n",
    "    # assign data\n",
    "    mydata = [[\"AGN\", np.mean(agn_cat['f1']), np.mean(agn_cat['precision']), np.mean(agn_cat['recall'])],\n",
    "          [\"SFG\", np.mean(sfg_cat['f1']), np.mean(sfg_cat['precision']), np.mean(sfg_cat['recall'])]]\n",
    "  \n",
    "    # create header\n",
    "    head = [\"Class\", \"F1_score\", \"Precision Score\", \"Recall score\" ]\n",
    "  \n",
    "    # display table\n",
    "    ML_results = tabulate(mydata, headers=head, tablefmt=\"grid\")\n",
    "    print('The table for ' + features)\n",
    "    print(ML_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64934f59-dca0-42e5-924a-3f78fd748bc2",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1cd6997-6546-447f-b5be-f49f4172da16",
   "metadata": {},
   "source": [
    "# DATA ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4fb530d-6228-423b-889b-710b1bfc684e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# THE SPATIAL DISTRIBUTION OF THE DATA\n",
    "\n",
    "pl.figure(figsize = (10, 10))\n",
    "pl.scatter( data['RADIORA'], data['RADIODEC'], c='r', lw=2, s=4)\n",
    "pl.ylabel('DEC (deg)',  fontweight ='bold', fontsize =18)\n",
    "pl.xlabel('RA (deg)',  fontweight ='bold', fontsize =18)\n",
    "pl.title('Spatial distribution of MIGHTEE Sources',  fontweight ='bold', fontsize =21)\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c552a5d-ad35-4438-97ac-489d35d0d7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important data columns for machine learning\n",
    "\n",
    "x_fea = ['COS_best_z_v5', 'SPLASH_1_FLUX', 'SPLASH_2_FLUX', 'SPLASH_3_FLUX', 'SPLASH_4_FLUX',\n",
    "            'L14','LIR_WHz','MASS_lephare', 'class_star','qir','flux_HSC-G','flux_HSC-R', 'flux_HSC-I','flux_HSC-Z',\n",
    "            'flux_J', 'flux_H', 'flux_Ks', 'flux_Y']\n",
    "y_fea = ['AGN', 'SFG', 'probSFG']\n",
    "\n",
    "\n",
    "trad_flux = data_processor(data, x_fea, y_fea)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4926e6bd-9ca8-48b8-bb22-79764616dcc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We rename AGN column to labels\n",
    "\n",
    "trad_flux.rename(columns = {'AGN':'class_labels', 'MASS_lephare':'Mstar'}, inplace = True)\n",
    "\n",
    "trad_flux_clean1 = trad_flux.dropna()\n",
    "trad_flux_clean = trad_flux_clean1.drop(['class_star', 'qir'], axis = 1)\n",
    "# trad_flux_clean1 = trad_flux\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88cee3fa-85b0-4298-8364-e93f00b88515",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using Pearson Correlation, ### You dont have to seperate the sources in a correlation matrix its called data leakage\n",
    "pl.figure(figsize=(12,10))\n",
    "cor = trad_flux_clean.corr()\n",
    "sns.heatmap(cor, annot=True, cmap=pl.cm.Reds)\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49561125-57b4-4fd9-b5d2-c596f8893917",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Making a pair plot using seaborn\n",
    "# pl.figure(figsize=(12,10))\n",
    "# sns.set(font_scale=2)\n",
    "# sns.set_style('ticks')\n",
    "# sns.pairplot(trad_flux_clean, hue=\"class_labels\", \n",
    "#              plot_kws = { 'edgecolor': 'k', 'alpha':0.4, 'lw':0.5, 's':20 }, size = 4)\n",
    "# pl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30b6af9-b2d3-432d-98b4-041025fa4bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "trad_flux_clean.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54eaead3-13e0-44d4-a9cf-4486ac370ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importance\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Machine learning models\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# Machine Learning models\n",
    "rf_model = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448d9bc9-204f-4016-8826-c3d2b58d7a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cols = ['COS_best_z_v5', 'SPLASH_1_FLUX', 'SPLASH_2_FLUX', 'SPLASH_3_FLUX', 'SPLASH_4_FLUX',\n",
    "            'L14','LIR_WHz','Mstar','flux_HSC-G','flux_HSC-R', 'flux_HSC-I','flux_HSC-Z',\n",
    "            'flux_J', 'flux_H', 'flux_Ks', 'flux_Y']\n",
    "\n",
    "x_cols_no_mstar = ['COS_best_z_v5', 'SPLASH_1_FLUX', 'SPLASH_2_FLUX', 'SPLASH_3_FLUX', 'SPLASH_4_FLUX',\n",
    "            'L14','LIR_WHz','flux_HSC-G','flux_HSC-R', 'flux_HSC-I','flux_HSC-Z',\n",
    "            'flux_J', 'flux_H', 'flux_Ks', 'flux_Y']\n",
    "x_trad = trad_flux_clean[x_cols]\n",
    "\n",
    "y_trad = trad_flux_clean['class_labels']\n",
    "\n",
    "# encoding target class\n",
    "y, clas = pd.factorize(y_trad)\n",
    "y_target = pd.DataFrame(y, columns = ['labels'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_trad, y_target, stratify = y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4324d8be-2483-4ec5-a789-8711b9242abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_trad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaedc33e-1134-4733-98be-22df1e9549ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def importance (rf_model, X_train, y_train):\n",
    "\n",
    "    iterations = np.arange(0, 50, 10)\n",
    "    mean_importances = []\n",
    "    std_importances = []\n",
    "    labels = []\n",
    "\n",
    "    rf_model.fit(X_train, np.ravel(y_train))\n",
    "    result = permutation_importance(rf_model, X_train, np.ravel(y_train), \n",
    "                                    n_repeats=10, random_state=42, \n",
    "                                    scoring = make_scorer(f1), n_jobs=2)\n",
    "\n",
    "    mean_importances = result.importances_mean\n",
    "    std_importances = result.importances_std\n",
    "\n",
    "    column_names = np.array(X_train.columns)\n",
    "\n",
    "    sorted_idx1 = mean_importances.argsort()\n",
    "    sorted_idx1 = sorted_idx1[::-1]\n",
    "    \n",
    "    x_pos = np.arange(len(column_names))\n",
    "    x_class = column_names[sorted_idx1]\n",
    "   \n",
    "    # Build the plot\n",
    "    fig, ax = pl.subplots(figsize = (12, 9))\n",
    "    ax.bar(x_pos, mean_importances[sorted_idx1], yerr=std_importances[sorted_idx1], align='center', alpha=0.5, color = 'green', ecolor='black', capsize=10)\n",
    "    ax.set_ylabel('Importance', fontsize=25)\n",
    "    ax.set_xticks(x_pos)\n",
    "    # ax.set_title('Permutation feature importances', fontsize = 15, fontweight ='bold')\n",
    "    ax.set_xticklabels(x_class, ha='right', rotation = 45, fontsize = 20) #, fontweight ='bold')\n",
    "    # ax.yaxis.grid(True)\n",
    "\n",
    "    # Save the figure and show\n",
    "    pl.setp(ax.get_yticklabels(), fontsize=20, ha=\"right\",\n",
    "         rotation_mode=\"anchor\")\n",
    "    pl.tight_layout()\n",
    "    pl.savefig('all_perm_importance.pdf')\n",
    "    pl.show()\n",
    "    \n",
    "    \n",
    "importance (rf_model, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2274db0-ddae-42cd-9e73-8974cd59807e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1641ff4b-414f-4397-9e38-e2287c3bac0c",
   "metadata": {},
   "source": [
    "# Machine Learning For Fluxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbde8c24-c1be-4abe-9f39-fe53a60e435f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_size = np.linspace(0.01, 0.99, 50)\n",
    "\n",
    "tr_size_02 = np.linspace(0.1, 0.99, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34edd5d7-236c-493d-adf6-33499e2e2fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting RF model\n",
    "# y_pred = knn.fit(x_train,y_train).predict(x_test)\n",
    "y_pred = rf_model.fit(X_train, np.ravel(y_train)).predict(X_test)\n",
    "\n",
    "\n",
    "# Compute confusion matrix for kNN classifier\n",
    "cm = metrics.confusion_matrix(y_test, y_pred)\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "pl.figure(figsize=(11,11))\n",
    "confusion_matrix(cm,classes=['AGN','SFG'], name = 'cm_fluxes', normalize=True,\n",
    "                      title='Normalized confusion matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54c9b3b-a7c3-4b89-9f24-9fd55c03c507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# agn_flux, sfg_flux = train_vs_score_cv( rf_model, X_train, y_train, tr_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e48fde-19df-46e6-b8f4-f54dff2f71b0",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea53031-2a85-464b-b4d2-bbdfffc8715a",
   "metadata": {},
   "source": [
    "# ANALYSIS FOR COLOURS AS INPUT FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27791d9a-aeab-46eb-b8f5-14212519c01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The trad Fluxes\n",
    "\n",
    "S8_S45 = np.log10( trad_flux_clean1['SPLASH_4_FLUX'] / trad_flux_clean1['SPLASH_2_FLUX'])\n",
    "S58_S36 = np.log10( trad_flux_clean1['SPLASH_3_FLUX'] / trad_flux_clean1['SPLASH_1_FLUX'])\n",
    "S45_S36 = np.log10( trad_flux_clean1['SPLASH_2_FLUX'] / trad_flux_clean1['SPLASH_1_FLUX'])\n",
    "\n",
    "\n",
    "trad_flux_clean1['log(S8/S45)'] = S8_S45\n",
    "trad_flux_clean1['log(S58/S36)'] = S58_S36\n",
    "# trad_flux_clean1['log(S58/S36)'] = S8_S45\n",
    "# trad_flux_clean1['log(S8/S45)'] = S58_S36\n",
    "trad_flux_clean1['log(S45/S36)'] = S45_S36"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00a6d1f-79d5-4816-9438-5fefaff862ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use color as features\n",
    "# magnitudes between two different filter bands.\n",
    "# features: [u-g, g-r, r-i, i-z]\n",
    "g_hsc = trad_flux_clean1['flux_HSC-G']\n",
    "r_hsc = trad_flux_clean1['flux_HSC-R']\n",
    "i_hsc = trad_flux_clean1['flux_HSC-I']\n",
    "z_hsc = trad_flux_clean1['flux_HSC-Z']   \n",
    "    \n",
    "# feaures\n",
    "g_r, r_i, i_z, g_i, g_z, r_z = np.array(np.log10(g_hsc / r_hsc)), np.array(np.log10(r_hsc /i_hsc)), np.array(np.log10(i_hsc /z_hsc)), np.array(np.log10(g_hsc / i_hsc)), np.array(np.log10(g_hsc / z_hsc)), np.array(np.log10(r_hsc / z_hsc))\n",
    "\n",
    "trad_flux_clean1['log(g/r)'], trad_flux_clean1['log(r/i)'], trad_flux_clean1['log(i/z)'], trad_flux_clean1['log(g/i)'], trad_flux_clean1['log(g/z)'], trad_flux_clean1['log(r/z)']  =  g_r, r_i, i_z, g_i, g_z, r_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7c199c-ccfd-45e2-8c85-746a43a215fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The NIR colours\n",
    "\n",
    "y_hsc = trad_flux_clean1['flux_Y']\n",
    "h_hsc = trad_flux_clean1['flux_H']\n",
    "k_hsc = trad_flux_clean1['flux_Ks']\n",
    "j_hsc = trad_flux_clean1['flux_J']\n",
    "    \n",
    "    \n",
    "# feaures\n",
    "y_j, j_h, h_k, y_h, y_k, j_k = np.array(np.log10(y_hsc / j_hsc)), np.array(np.log10(j_hsc /h_hsc)), np.array(np.log10(h_hsc / k_hsc)), np.array(np.log10(y_hsc / h_hsc)), np.array(np.log10(y_hsc / k_hsc)), np.array(np.log10(j_hsc / k_hsc))\n",
    "# hsc_columns = ['y/j', 'j/h', 'h/k']\n",
    "trad_flux_clean1['log(Y/J)'], trad_flux_clean1['log(J/H)'], trad_flux_clean1['$log(H/K_{s})$'], trad_flux_clean1['log(Y/H)'], trad_flux_clean1['$log(Y/K_{s})$'],trad_flux_clean1['$log(J/K_{s})$'] =  y_j, j_h, h_k,y_h, y_k, j_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1688674-e444-4882-bef2-9395c0a55ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "trad_flux_clean1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2da2761-b305-4857-9c41-cda1dc2b37d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors_data = trad_flux_clean1.drop(x_cols_no_mstar, axis = 1)\n",
    "colors_data1 = colors_data.replace(-np.inf, np.nan, regex=True)\n",
    "colors = colors_data1.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a608140c-599e-4b6c-96c2-192197b00533",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using Pearson Correlation, ### You dont have to seperate the sources in a correlation matrix its called data leakage\n",
    "pl.figure(figsize=(12,10))\n",
    "cor1 = colors.corr()\n",
    "sns.heatmap(cor1, annot=True, cmap=pl.cm.Reds)\n",
    "pl.savefig('pearson_corr')\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986e9296-5f58-4a09-9652-dbfa4f90252f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Making a pair plot using seaborn\n",
    "pl.figure(figsize=(12,10))\n",
    "sns.set(font_scale=2)\n",
    "sns.set_style('ticks')\n",
    "sns.pairplot(colors, hue=\"class_labels\", \n",
    "             plot_kws = { 'edgecolor': 'k', 'alpha':0.4, 'lw':0.5, 's':20 }, size = 4)\n",
    "pl.savefig('colors_pairplot')\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d1c83f-d1ee-4ae4-b638-165c9079bcbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(colors.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c709123a-250d-4a46-98dd-44701d902277",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd65f43-f135-4e58-9730-2acf2d75ba4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_color = colors.drop('class_labels', axis = 1)\n",
    "\n",
    "y_color = colors['class_labels']\n",
    "\n",
    "# encoding target class\n",
    "y1, clas1 = pd.factorize(y_color)\n",
    "target = pd.DataFrame(y1, columns = ['labels'])\n",
    "\n",
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(x_color, target, stratify = y1, test_size=0.15, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9433a810-f76e-4259-a184-508e1fa034e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train1_final= X_train1.rename(columns = {'Mstar':'$log (M_{star})$', \n",
    "                               'qir':'$q_\\mathrm{IR}$', \n",
    "                               'log(S8/S45)':'$log(S_{8.0}/S_{4.5})$', \n",
    "                               'log(S58/S36)': '$log(S_{5.8}/S_{3.6})$',\n",
    "                               'log(S45/S36)': '$log(S_{4.5}/S_{3.6})$'\n",
    "                              } )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a395c7e5-484b-4658-a491-9545ea99355a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train1_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714c60aa-7eb8-49ea-af65-262d1b771d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the dataframe as raw_data\n",
    "X_train1_final.reset_index(inplace = True)\n",
    "X_train1_final.to_csv('scaled_all_colors.csv', index = False, header=True)\n",
    "\n",
    "# We split the data into training and test size for further analysis\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from sklearn import preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "# scaler = MaxAbsScaler()\n",
    "# scaler = StandardScaler()\n",
    "\n",
    "# labels = catalogue['class_labels']\n",
    "# X = catalogue.drop(['class_labels'], axis = 1)\n",
    "\n",
    "X_norm = scaler.fit_transform(X_train1_final)\n",
    "scaled_X = pd.DataFrame( X_norm, columns = X_train1_final.columns)\n",
    "\n",
    "scaled_X = scaled_X.drop(\"index\", axis=\"columns\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83df94fd-c139-4ef6-92cb-6cce22e32ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaled_X = scaled_X.drop(\"level_0\", axis=\"columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619678b6-0778-4e57-bbae-d8f9f43679ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_X.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7783368e-a1de-4902-b4c3-87c03daaedb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "importance (rf_model, scaled_X, y_train1) \n",
    "\n",
    "# importance (rf_model, X_train1_final, y_train1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c242f9e-11c9-4a8d-a8e1-ae6cd04cae6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# agn_flux1, sfg_flux1 = train_vs_score_cv( rf_model, X_train1, y_train1, tr_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756b3d1f-55ec-4d1d-aef8-55b5024e9461",
   "metadata": {},
   "outputs": [],
   "source": [
    "# colors.to_csv('all_colors.csv', index = False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70548ea3-ad33-4e4a-8ca1-b3bae8b7a3d6",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af229163-304b-44ad-86c2-2d4a9d4a3ed6",
   "metadata": {},
   "source": [
    "# UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3743608-7dec-461f-8687-99b7e2a2b18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from umap import UMAP\n",
    "\n",
    "# plt.figure(figsize=(20,15))\n",
    "# model = UMAP(n_neighbors = 15, min_dist = 0.25, n_components = 2, verbose = True)\n",
    "# umap = model.fit_transform(X_train1)\n",
    "# plt.scatter(umap[:, 0], umap[:, 1], c = y_train.astype(int), cmap = 'tab10', s = 50)\n",
    "# plt.title('UMAP', fontsize = 20)\n",
    "# plt.xlabel(\"UMAP1\", fontsize = 20)\n",
    "# plt.ylabel(\"UMAP2\", fontsize = 20)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0180d3-926c-448f-b365-409dd54e8ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use missingno to view the missingness in each feature in the data\n",
    "# import missingno as msno\n",
    "\n",
    "# fig = msno.bar(x_color,figsize=(10, 8), color = 'g')\n",
    "\n",
    "# fig_copy = fig.get_figure()\n",
    "# fig_copy.savefig('all-missingno.pdf', bbox_inches = 'tight')\n",
    "\n",
    "# Generate the missingno bar plot\n",
    "# ax = msno.bar(x_color, color = 'g')\n",
    "\n",
    "# # Customize: hide the right vertical axis (right spine)\n",
    "# ax.spines['right'].set_visible(False)\n",
    "\n",
    "# # Further customization (optional)\n",
    "# ax.set_xlabel('Features', fontsize=12)\n",
    "# ax.set_ylabel('Count of non-null values', fontsize=12)\n",
    "# ax.set_title('Missing Data Overview', fontsize=15)\n",
    "\n",
    "# # Show the plot\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73ed833-aa15-424b-8536-02bbe7e8c172",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ASTRO-PY3",
   "language": "python",
   "name": "astro-py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
