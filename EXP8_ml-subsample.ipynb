{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ff1e47-9770-4eb1-83ef-873abc2c892f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading necessary modlules\n",
    "import os \n",
    "import os.path as osp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as pl\n",
    "pl.rcParams['axes.labelsize'] = 16\n",
    "pl.rcParams['axes.titlesize'] = 16\n",
    "import itertools\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import timeit\n",
    "import sklearn\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score \n",
    "# Scores\n",
    "from sklearn.metrics import accuracy_score as accuracy\n",
    "from sklearn.metrics import f1_score as f1\n",
    "from sklearn.metrics import recall_score as recall\n",
    "from sklearn.metrics import precision_score as precision\n",
    "from sklearn.metrics import classification_report, make_scorer\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "# ML models  \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import xgboost\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from pprint import pprint\n",
    "pl.style.use('seaborn-ticks')\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d82901c-3121-4f78-af05-bf5e57e50de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The train data\n",
    "X_data = pd.read_csv('X_train_new.csv')\n",
    "y_data = pd.read_csv('y_train_new.csv')\n",
    "\n",
    "# The Unseen test data\n",
    "X_test = pd.read_csv('X_test_new.csv')\n",
    "y_test = pd.read_csv('y_test_new.csv')\n",
    "\n",
    "# y = y_data['labels']\n",
    "y_tr = np.array(y_data).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a29c841-0953-4e40-b0c7-b4e1c213ee8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2feb4aea-0042-4d0c-aafc-6996022f3f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_vald, y_train, y_vald = train_test_split(X_data, y_data, stratify = y_data, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43bc54ab-3187-4f14-b9f1-7f2ebfa81ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## random forest (RF)\n",
    "# The Random Hyper parameter Grid\n",
    "\n",
    "# number of trees in the forest\n",
    "n_estimators = [50, 100, 150]\n",
    "\n",
    "# Number of feature to consider at every split\n",
    "max_features = [2, 3]\n",
    "\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [5, 10]\n",
    "\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5]\n",
    "\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 3]\n",
    "\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "\n",
    "\n",
    "# Create the random grid\n",
    "rf_par = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth}\n",
    "               # 'min_samples_split': min_samples_split,\n",
    "               # 'min_samples_leaf': min_samples_leaf,\n",
    "               # 'bootstrap': bootstrap}\n",
    "\n",
    "\n",
    "\n",
    "rf_model= RandomForestClassifier(random_state=1)\n",
    "rf_par = dict(n_estimators=n_estimators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb01abe-41b7-42bd-8537-e8a5bbe7d24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_vald_test = pd.concat([X_vald, X_test], sort=False)\n",
    "\n",
    "# y_vald_df = pd.DataFrame(y_vald, columns = ['labels'])\n",
    "y_vald_test = pd.concat([y_vald, y_test], sort=False)\n",
    "\n",
    "# X_vald_test['label'] =  y_vald_test\n",
    "# X_vald_test.reset_index(drop=True)\n",
    "# y_vald_test.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0be13e-9847-4f99-b344-c45dcba88877",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLASSIFIER AND METRICS ALGORITHMS\n",
    "\n",
    "# Source Classification\n",
    "start_time = timeit.default_timer()\n",
    "\n",
    "rf_model.fit(X_train, np.array(y_train).ravel())  \n",
    "\n",
    "y_pred = rf_model.predict(X_vald_test)\n",
    "y_pred_vald = rf_model.predict(X_vald)\n",
    "y_pred_test = rf_model.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "proba = rf_model.predict_proba(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b0af87-2685-4bae-97f1-9ce5f0ef6dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(cm, classes, \n",
    "                        name = '',\n",
    "                        normalize=False,\n",
    "                        title='Confusion Matrix',\n",
    "                        cmap=pl.cm.Greens):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    pl.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    pl.title(title, fontsize=26)\n",
    "    pl.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    pl.xticks(tick_marks, classes, fontsize=20,rotation=45)\n",
    "    pl.yticks(tick_marks, classes, fontsize=20)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "    \n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        pl.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\",\n",
    "                 fontsize=28)\n",
    "\n",
    "    pl.tight_layout()\n",
    "    pl.ylabel('True Label', fontsize=20)\n",
    "    pl.xlabel('Predicted Label', fontsize=20)\n",
    "    pl.savefig(name)\n",
    "    pl.show()# Feature importance for the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de979ca-4a06-4165-b8e1-6e5f2e0c193e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute confusion matrix for kNN classifier\n",
    "cm_test = metrics.confusion_matrix(y_test, y_pred_test)\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "pl.figure(figsize=(11,11))\n",
    "confusion_matrix(cm_test,classes=['AGN','SFG'], name = 'cm_fluxes', normalize=True,\n",
    "                      title='Normalized confusion matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8d37dd-623d-40bd-a975-80d4712e2404",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute confusion matrix for kNN classifier\n",
    "cm = metrics.confusion_matrix(y_vald_test, y_pred)\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "pl.figure(figsize=(11,11))\n",
    "confusion_matrix(cm,classes=['AGN','SFG'], name = 'cm_fluxes', normalize=True,\n",
    "                      title='Normalized confusion matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38bfdc62-5604-4f61-a5c1-eaea34166947",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute confusion matrix for kNN classifier\n",
    "cm_noxray = metrics.confusion_matrix(y_vald, y_pred_vald)\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "pl.figure(figsize=(11,11))\n",
    "confusion_matrix(cm_noxray,classes=['AGN','SFG'], name = 'cm_fluxes', normalize=True,\n",
    "                      title='Normalized confusion matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705a6ffd-1c84-41fb-9ced-8b6806a2c1da",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34711cbb-d3ce-4b47-ab87-79761ba84fc4",
   "metadata": {},
   "source": [
    "## Experiments to produce F1, Precision and Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523d1c0d-69f3-475b-872b-0433a27801d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data = pd.read_csv('final-train-test/X_train.csv')\n",
    "y_data = pd.read_csv('final-train-test/y_train.csv')\n",
    "\n",
    "# The Unseen test data\n",
    "X_test = pd.read_csv('final-train-test/X_test.csv')\n",
    "y_test = pd.read_csv('final-train-test/y_test.csv')\n",
    "\n",
    "referece_test = pd.read_csv('final-train-test/original_test_df.csv')\n",
    "\n",
    "\n",
    "y = y_data['labels']\n",
    "\n",
    "# drop catid\n",
    "X_data = X_data.drop([\"CATID\"], axis='columns')\n",
    "X_test = X_test.drop([\"CATID\"], axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f3030d-2c27-46c6-8663-8fbc71093d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # The train data\n",
    "# X_data_m = pd.read_csv('X_train.csv')\n",
    "# y_data_m = pd.read_csv('y_train.csv')\n",
    "\n",
    "# # The Unseen test data\n",
    "# X_test_m = pd.read_csv('X_test.csv')\n",
    "# y_test_m = pd.read_csv('y_test.csv')\n",
    "\n",
    "\n",
    "# # With CatID\n",
    "# # X_data_tb = pd.read_csv('X_train_table.csv')\n",
    "# # y_data_tb = pd.read_csv('y_train_table.csv')\n",
    "# # X_tb_train = X_data_tb.drop(['index','CATID'], axis = 1)\n",
    "# # # X_tb_train = X_tb_train.dropna()\n",
    "\n",
    "# # X_test_tb = pd.read_csv('X_test_table.csv')\n",
    "# # y_test_tb = pd.read_csv('y_test_table.csv')\n",
    "# # X_tb_test = X_test_tb.drop(['index','CATID'], axis = 1)\n",
    "# # X_tb_test = X_tb_test.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b3f7e9-70e0-49d5-a3cc-0d5377bea71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.read_csv('raw_data.csv')\n",
    "# data = data.dropna()\n",
    "# # data = data.drop(['index','CATID'], axis = 1)\n",
    "# data = data.drop(['CATID'], axis = 1)\n",
    "\n",
    "\n",
    "# labels = data['class_labels']\n",
    "# X_tb = data.drop(['class_labels'], axis = 1)\n",
    "\n",
    "# # encoding target class\n",
    "# y_tb, clas_tb = pd.factorize(labels) #getting the class 0 = agn, 1 =notagn, 2 = no class\n",
    "# y_target_tb = pd.DataFrame(y_tb, columns = ['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d9e82c-fb86-432a-94ac-34031e6f0fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a769c1-3804-414a-b430-9c522b840c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tb, X_test_tb, y_train_tb, y_test_tb = train_test_split(X_data, y_data, stratify = y_data, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a379c62-4d92-4ec2-9b5a-8ec801f048c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(X_train_tb.isna().sum().sum())\n",
    "# print(X_test_tb.isna().sum().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb04464-c428-4faf-9816-13e5f400672c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_tb_noindex = X_train_tb.drop(['index'], axis = 1)\n",
    "\n",
    "# X_test_tb_noindex = X_test_tb.drop(['index'], axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1cab42-bab7-4332-9cbe-38eb6752d628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_score(model, X_train, X_test, y_train, y_test):\n",
    "\n",
    "#     y_tr = np.array(y_data).ravel()\n",
    "    \n",
    "#     # CLASSIFIER AND METRICS ALGORITHMS\n",
    "#     model.fit(X_train, np.array(y_tr).ravel())  \n",
    "    \n",
    "#     # Predict\n",
    "#     y_pred = rf_model.predict(X_test)\n",
    "\n",
    "#     #scores\n",
    "#     f1_res = f1(y_tr, y_pred)\n",
    "#     recall_res = recall(y_tr, y_pred)\n",
    "#     precision_res = prcision(y_tr, y_pred)\n",
    "\n",
    "#     #print the results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983a06e5-96ec-47a0-88a1-e647d0a88f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Original dataset\n",
    "\n",
    "X_train = pd.read_csv('X_train.csv')\n",
    "y_train = pd.read_csv('y_train.csv')\n",
    "\n",
    "# The Unseen test data\n",
    "X_test = pd.read_csv('X_test.csv')\n",
    "y_test = pd.read_csv('y_test.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26bff9f0-22cf-4a9c-915d-aacaabe1d7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import cross_val_score\n",
    "# from sklearn.metrics import f1_score as f1, recall_score as recall, precision_score as precision\n",
    "# import numpy as np\n",
    "\n",
    "def get_score(models, X_train, X_test, y_train, y_test, cv=5, save_to_file=False, filename='model_scores.txt', class_to_predict=1):\n",
    "    \"\"\"\n",
    "    Evaluate multiple models using cross-validation and print/save F1, Recall, and Precision scores.\n",
    "    Returns the trained models along with evaluation results.\n",
    "    \n",
    "    Parameters:\n",
    "    - models: List of models to evaluate.\n",
    "    - X_train, X_test, y_train, y_test: Training and testing datasets.\n",
    "    - cv: Number of folds for cross-validation (e.g., 3, 5, 10).\n",
    "    - save_to_file: Boolean, whether to save the results to a file.\n",
    "    - filename: Name of the file to save the results.\n",
    "    \n",
    "    Returns:\n",
    "    - results: List of dictionaries containing evaluation results and trained models.\n",
    "    \"\"\"\n",
    "    \n",
    "    y_tr = np.array(y_train).ravel()\n",
    "    y_te = np.array(y_test).ravel()\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for model in models:\n",
    "        # Perform cross-validation\n",
    "        f1_scores = cross_val_score(model, X_train, y_tr, cv=cv, scoring='f1')\n",
    "        recall_scores = cross_val_score(model, X_train, y_tr, cv=cv, scoring='recall')\n",
    "        precision_scores = cross_val_score(model, X_train, y_tr, cv=cv, scoring='precision')\n",
    "        \n",
    "        # Compute mean and standard deviation of scores\n",
    "        f1_mean, f1_std = f1_scores.mean(), f1_scores.std()\n",
    "        recall_mean, recall_std = recall_scores.mean(), recall_scores.std()\n",
    "        precision_mean, precision_std = precision_scores.mean(), precision_scores.std()\n",
    "        \n",
    "        # Train the model on the full training set\n",
    "        model.fit(X_train, y_tr)\n",
    "        \n",
    "        # Predict on the test set\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Compute test scores\n",
    "        f1_test = f1(y_te, y_pred, zero_division=0)\n",
    "        recall_test = recall(y_te, y_pred, zero_division=0)\n",
    "        precision_test = precision(y_te, y_pred, zero_division=0)\n",
    "        \n",
    "        # Store results and the trained model\n",
    "        results.append({\n",
    "            'model': model.__class__.__name__,\n",
    "            'trained_model': model,  # Store the trained model\n",
    "            'f1_mean': f1_mean,\n",
    "            'f1_std': f1_std,\n",
    "            'recall_mean': recall_mean,\n",
    "            'recall_std': recall_std,\n",
    "            'precision_mean': precision_mean,\n",
    "            'precision_std': precision_std,\n",
    "            'f1_test': f1_test,\n",
    "            'recall_test': recall_test,\n",
    "            'precision_test': precision_test,\n",
    "            'y_pred': y_pred\n",
    "        })\n",
    "        \n",
    "        # Print results\n",
    "        print(f\"Model: {model.__class__.__name__}\")\n",
    "        print(f\"CV F1 Score: {f1_mean:.4f} (±{f1_std:.4f})\")\n",
    "        print(f\"CV Recall: {recall_mean:.4f} (±{recall_std:.4f})\")\n",
    "        print(f\"CV Precision: {precision_mean:.4f} (±{precision_std:.4f})\")\n",
    "        print(f\"Test F1 Score: {f1_test:.4f}\")\n",
    "        print(f\"Test Recall: {recall_test:.4f}\")\n",
    "        print(f\"Test Precision: {precision_test:.4f}\")\n",
    "        print(\"-\" * 40)\n",
    "    \n",
    "    # Save results to file if requested\n",
    "    if save_to_file:\n",
    "        with open(filename, 'w') as f:\n",
    "            for res in results:\n",
    "                f.write(f\"Model: {res['model']}\\n\")\n",
    "                f.write(f\"CV F1 Score: {res['f1_mean']:.4f} (±{res['f1_std']:.4f})\\n\")\n",
    "                f.write(f\"CV Recall: {res['recall_mean']:.4f} (±{res['recall_std']:.4f})\\n\")\n",
    "                f.write(f\"CV Precision: {res['precision_mean']:.4f} (±{res['precision_std']:.4f})\\n\")\n",
    "                f.write(f\"Test F1 Score: {res['f1_test']:.4f}\\n\")\n",
    "                f.write(f\"Test Recall: {res['recall_test']:.4f}\\n\")\n",
    "                f.write(f\"Test Precision: {res['precision_test']:.4f}\\n\")\n",
    "                f.write(\"-\" * 40 + \"\\n\")\n",
    "        print(f\"Results saved to {filename}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "# Example usage:\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# models = [RandomForestClassifier(), LogisticRegression()]\n",
    "# results = get_score(models, X_train, X_test, y_train, y_test, cv=5, save_to_file=True)\n",
    "\n",
    "# Accessing trained models:\n",
    "# trained_rf = results[0]['trained_model']  # Trained RandomForestClassifier\n",
    "# trained_lr = results[1]['trained_model']  # Trained LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3bcda8-d4d7-483a-a4a4-0871f30dee18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ML Models\n",
    "rf_model2 = RandomForestClassifier(random_state=1)\n",
    "svm_model = SVC(kernel='linear')\n",
    "knn_model = KNeighborsClassifier()\n",
    "lr_model = LogisticRegression()\n",
    "xgb_model = xgboost.XGBClassifier(use_label_encoder=False, eval_metric='rmse', n_jobs=-1 )\n",
    "\n",
    "models = [lr_model, svm_model, knn_model, rf_model2, xgb_model]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3700063-310b-49d0-bafa-f7952ba0cd70",
   "metadata": {},
   "source": [
    "### WRITE UP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e864f6c2-c4a8-410b-9020-d2c4744c9efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spliting data 20 % test / 80 % train\n",
    "# X_train_splt02, X_test_splt02, y_train_splt02, y_test_splt02 = train_test_split(X_train_tb_noindex, y_train_tb, stratify = y_train_tb, test_size=0.8, random_state=42)\n",
    "X_train_splt02, X_test_splt02, y_train_splt02, y_test_splt02 = train_test_split(X_train, y_train, stratify = y_train, test_size=0.8, random_state=42)\n",
    "\n",
    "results = get_score(models, X_train_splt02, X_test_splt02, y_train_splt02, y_test_splt02, save_to_file=True, filename='ml_review_results/model_scores_02.txt', class_to_predict = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf473e9-c9d7-4eb8-84a2-9d26d729915f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# models = [lr_model, svm_model, knn_model, rf_model2, xgb_model]\n",
    "\n",
    "# Access trained models\n",
    "trained_lr = results[0]['trained_model']  # Trained RandomForestClassifier\n",
    "trained_svm = results[1]['trained_model']  # Trained LogisticRegression\n",
    "trained_knn = results[2]['trained_model']  # Trained LogisticRegression\n",
    "trained_rf = results[3]['trained_model']  # Trained LogisticRegression\n",
    "trained_xgb = results[4]['trained_model']  # Trained LogisticRegression\n",
    "\n",
    "# Use the trained models for further predictions\n",
    "new_predictions_lr = trained_lr.predict(X_test_tb_noindex)\n",
    "new_predictions_svm = trained_svm.predict(X_test_tb_noindex)\n",
    "new_predictions_knn = trained_knn.predict(X_test_tb_noindex)\n",
    "new_predictions_rf = trained_rf.predict(X_test_tb_noindex)\n",
    "new_predictions_xgb = trained_xgb.predict(X_test_tb_noindex)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b343fce3-1e31-465f-9f3b-e29e9accab12",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(new_predictions_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ecb9c1a-4d76-41c9-8176-572b952dc161",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_test_tb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e7edce-be25-4555-9b0c-92bdb154cbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def arrays_to_df(predicted, column_names):\n",
    "    \"\"\"\n",
    "    Convert a list of arrays into a pandas DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    arrays (list of array-like): List of 1D arrays or lists, each representing a column.\n",
    "    columns (list of str): List of column names.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A DataFrame constructed from the given arrays.\n",
    "    \"\"\"\n",
    "    if len(predicted) != len(column_names):\n",
    "        raise ValueError(\"Number of arrays must match the number of column names\")\n",
    "\n",
    "    # Transpose the list of arrays to match DataFrame structure\n",
    "    data = np.column_stack(predicted)  # Ensures alignment of data\n",
    "    return pd.DataFrame(data, columns=column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd90b56-8c32-471e-bf90-4b286d198941",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_out_put = [new_predictions_lr, new_predictions_svm, new_predictions_knn, new_predictions_rf, new_predictions_xgb]\n",
    "col_names = ['lr', 'svm', 'knn', 'rf', 'xgb']\n",
    "\n",
    "outcomes = arrays_to_df(predicted_out_put, col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b39fa5-b11b-4e21-9f0d-dfb249ee3559",
   "metadata": {},
   "outputs": [],
   "source": [
    "outcomes['index'] = np.array(X_test_tb['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7febb8a3-b531-4ba4-b9a4-5624ceef9eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_test_df = pd.merge(X_test_tb, outcomes, on='index', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed78b1a-2551-4efa-96df-24b7363b7afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_test_df['true_label'] = np.array(y_test_tb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745f0075-1ad7-4977-a1aa-a88fbcb3ee9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog = pd.read_csv('raw_data.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab43e9d-b705-465d-a27d-8c55f2fa3446",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_test_df_withID = pd.merge(catalog, full_test_df, on='index', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15a3bc7-2926-438d-9392-bc7f8e552dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import astropy.io.fits as fits\n",
    "from astropy.table import Table\n",
    "\n",
    "file = osp.join(\"COSMOSXMATCH+classes_040422_withphotometry.fits\")\n",
    "\n",
    "hdu_list = fits.open(file)\n",
    "\n",
    "evt_data = Table(hdu_list[1].data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff67129-725a-438d-9233-1f2aa0fddba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mightee_data = evt_data.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b3059b-c08f-4a7e-84ad-2344d11093c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_test_df_withID_all_class = pd.merge(full_test_df_withID, mightee_data, on='CATID', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b5dd7e-5ca6-4948-9f60-7672e76c9b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_test_df_withID_all_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf1e195-7848-4282-a820-17041e2b9bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_test_df_withID_all_class.to_csv('classified_data_08.csv', index = False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a4d4f1-512a-4dfa-b725-f0675ce4b224",
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog_agn_sfg_xray = full_test_df_withID_all_class[full_test_df_withID_all_class['XAGN'] == True]\n",
    "catalog_agn_sfg_vlbi = full_test_df_withID_all_class[full_test_df_withID_all_class['VLBAAGN'] == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39554c7-bc9b-4f6b-9190-d1dab933dcc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(catalog_agn_sfg_xray.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38590db-f0fb-4364-b9fc-fe2ff7f769a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names_xray = ['CATID', 'XAGN', 'lr', 'svm', 'knn', 'rf', 'xgb', 'true_label']\n",
    "col_names_vlbi = ['CATID', 'VLBAAGN', 'lr', 'svm', 'knn', 'rf', 'xgb', 'true_label']\n",
    "\n",
    "catalog_agn_sfg_xray = catalog_agn_sfg_xray[col_names_xray]\n",
    "catalog_agn_sfg_vlbi = catalog_agn_sfg_vlbi[col_names_vlbi]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28efd8f9-b30f-449d-867d-899c3a8654b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog_agn_sfg_xray.to_csv('ml_review_results/catalog_agn_sfg_xray_02.csv', index = False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61aabf6f-838e-4494-b803-e3b186953f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog_agn_sfg_vlbi.to_csv('ml_review_results/catalog_agn_sfg_vlbi_02.csv', index = False, header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbf32c1-f65a-4929-b580-11ce753a1f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## DONE FISRT PART"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99fb515f-5976-4398-9161-dd70b70133b0",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cab8553-698f-45cb-a0e2-582e442d576f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # spliting data 80 % test / 20 % train\n",
    "# X_train_splt08, X_test_splt08, y_train_splt08, y_test_splt08 = train_test_split(X_data_m, y_data_m, stratify = y_data_m, test_size=0.8, random_state=42)\n",
    "\n",
    "# all_res_splt = get_score(models, X_train_splt08, X_test_splt08, y_train_splt08, y_test_splt08, save_to_file=False, filename='model_scores.txt', class_to_predict = 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4297595b-55b6-446c-b054-a00b35b8d849",
   "metadata": {},
   "source": [
    "### Full Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d4091b-1e88-4234-925a-c83142498fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_results = get_score(models, X_data_m, X_test_m, y_data_m, y_test_m, save_to_file=False, filename='model_scores.txt', class_to_predict = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14f48f0-3f97-4625-a707-8a177fcad671",
   "metadata": {},
   "outputs": [],
   "source": [
    "# noxrayvlbi_data = get_score(models, X_train, X_vald, y_train, y_vald, save_to_file=False, filename='model_scores.txt', class_to_predict = 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e035b6fa-8664-4e9d-9257-f6e2ae6bace9",
   "metadata": {},
   "source": [
    "### Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89fefac4-ed61-425a-9c91-bb5833a79f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Resampling the SFGs to equal the galaxies\n",
    "# under_sampler = RandomUnderSampler(random_state=42)\n",
    "\n",
    "\n",
    "# X_res, y_res = under_sampler.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f228e8bb-9a68-4b18-9549-78742c7528c3",
   "metadata": {},
   "source": [
    "### ONLY VLBI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c2a251-cbd2-4219-ab37-197c97b8111a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # The Unseen test data\n",
    "# X_vlba = pd.read_csv('X_vlba.csv')\n",
    "# y_vlba = pd.read_csv('y_vlba.csv')\n",
    "\n",
    "# # y = y_data['labels']\n",
    "# # y_tr = np.array(y_data).ravel()\n",
    "# y_vlb, clas_vlb = pd.factorize(y_vlba[\"class_labels\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cefd0dee-f015-4832-9c57-2210395d5188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_vlb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857edf59-c116-496d-b71a-05f6da507af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_vlba = get_score(models, X_res, X_vlba, y_res, y_vlb, save_to_file=False, filename='model_scores_xray.txt', class_to_predict = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4bc248f-e1ef-468b-8782-928746b2dc99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_vlba"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef8aefa-7562-4d35-95e3-0862536446dc",
   "metadata": {},
   "source": [
    "### ONLY XRAY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a745df-8c9d-4939-aa5d-d7673bde3680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # The Unseen test data\n",
    "# X_xray = pd.read_csv('X_xray.csv')\n",
    "# y_xray = pd.read_csv('y_xray.csv')\n",
    "\n",
    "# # y = y_data['labels']\n",
    "# # y_tr = np.array(y_data).ravel()\n",
    "# y_ray, clas_ray = pd.factorize(y_xray[\"class_labels\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a564ad-583e-4c6a-abba-55fb02c299e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_xray = get_score(models, X_res, X_xray, y_res, y_ray, save_to_file=False, filename='model_scores_vlba.txt', class_to_predict = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad14b9d-c691-4f34-9b34-57fc84266a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OLD FUNCTIONS\n",
    "\n",
    "# def get_score(models, X_train, X_test, y_train, y_test, cv=5, save_to_file=False, filename='model_scores.txt', class_to_predict=1):\n",
    "#     \"\"\"\n",
    "#     Evaluate multiple models using cross-validation and print/save F1, Recall, and Precision scores.\n",
    "    \n",
    "#     Parameters:\n",
    "#     - models: List of models to evaluate.\n",
    "#     - X_train, X_test, y_train, y_test: Training and testing datasets.\n",
    "#     - cv: Number of folds for cross-validation (e.g., 3, 5, 10).\n",
    "#     - save_to_file: Boolean, whether to save the results to a file.\n",
    "#     - filename: Name of the file to save the results.\n",
    "#     \"\"\"\n",
    "#     y_tr = np.array(y_train).ravel()\n",
    "#     y_te = np.array(y_test).ravel()\n",
    "    \n",
    "#     results = []\n",
    "    \n",
    "#     for model in models:\n",
    "#         # Perform cross-validation\n",
    "#         f1_scores = cross_val_score(model, X_train, y_tr, cv=cv, scoring='f1')\n",
    "#         recall_scores = cross_val_score(model, X_train, y_tr, cv=cv, scoring='recall')\n",
    "#         precision_scores = cross_val_score(model, X_train, y_tr, cv=cv, scoring='precision')\n",
    "        \n",
    "#         # Compute mean and standard deviation of scores\n",
    "#         f1_mean, f1_std = f1_scores.mean(), f1_scores.std()\n",
    "#         recall_mean, recall_std = recall_scores.mean(), recall_scores.std()\n",
    "#         precision_mean, precision_std = precision_scores.mean(), precision_scores.std()\n",
    "        \n",
    "#         # Train the model on the full training set\n",
    "#         model.fit(X_train, y_tr)\n",
    "        \n",
    "#         # Predict on the test set\n",
    "#         y_pred = model.predict(X_test)\n",
    "        \n",
    "#         # Compute test scores\n",
    "#         f1_test = f1(y_te, y_pred, zero_division=0)\n",
    "#         recall_test = recall(y_te, y_pred, zero_division=0)\n",
    "#         precision_test = precision(y_te, y_pred, zero_division=0)\n",
    "        \n",
    "#         # Store results\n",
    "#         results.append({\n",
    "#             'model': model.__class__.__name__,\n",
    "#             'f1_mean': f1_mean,\n",
    "#             'f1_std': f1_std,\n",
    "#             'recall_mean': recall_mean,\n",
    "#             'recall_std': recall_std,\n",
    "#             'precision_mean': precision_mean,\n",
    "#             'precision_std': precision_std,\n",
    "#             'f1_test': f1_test,\n",
    "#             'recall_test': recall_test,\n",
    "#             'precision_test': precision_test,\n",
    "#             'y_pred': y_pred\n",
    "#         })\n",
    "        \n",
    "#         # Print results\n",
    "#         print(f\"Model: {model.__class__.__name__}\")\n",
    "#         print(f\"CV F1 Score: {f1_mean:.4f} (±{f1_std:.4f})\")\n",
    "#         print(f\"CV Recall: {recall_mean:.4f} (±{recall_std:.4f})\")\n",
    "#         print(f\"CV Precision: {precision_mean:.4f} (±{precision_std:.4f})\")\n",
    "#         print(f\"Test F1 Score: {f1_test:.4f}\")\n",
    "#         print(f\"Test Recall: {recall_test:.4f}\")\n",
    "#         print(f\"Test Precision: {precision_test:.4f}\")\n",
    "#         print(\"-\" * 40)\n",
    "    \n",
    "#     # Save results to file if requested\n",
    "#     if save_to_file:\n",
    "#         with open(filename, 'w') as f:\n",
    "#             for res in results:\n",
    "#                 f.write(f\"Model: {res['model']}\\n\")\n",
    "#                 f.write(f\"CV F1 Score: {res['f1_mean']:.4f} (±{res['f1_std']:.4f})\\n\")\n",
    "#                 f.write(f\"CV Recall: {res['recall_mean']:.4f} (±{res['recall_std']:.4f})\\n\")\n",
    "#                 f.write(f\"CV Precision: {res['precision_mean']:.4f} (±{res['precision_std']:.4f})\\n\")\n",
    "#                 f.write(f\"Test F1 Score: {res['f1_test']:.4f}\\n\")\n",
    "#                 f.write(f\"Test Recall: {res['recall_test']:.4f}\\n\")\n",
    "#                 f.write(f\"Test Precision: {res['precision_test']:.4f}\\n\")\n",
    "#                 f.write(\"-\" * 40 + \"\\n\")\n",
    "#         print(f\"Results saved to {filename}\")\n",
    "\n",
    "#     return results\n",
    "\n",
    "# # Example usage:\n",
    "# # from sklearn.ensemble import RandomForestClassifier\n",
    "# # from sklearn.linear_model import LogisticRegression\n",
    "# # models = [RandomForestClassifier(), LogisticRegression()]\n",
    "# # get_score(models, X_train, X_test, y_train, y_test, cv=5, save_to_file=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788e5503-9539-41f2-95a4-72e74cdd69a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ASTRO-PY3",
   "language": "python",
   "name": "astro-py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
