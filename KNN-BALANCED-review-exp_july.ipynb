{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572efc4d-e219-496b-b443-1bac2ca718de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading necessary modlules\n",
    "import numpy as np\n",
    "import os \n",
    "import os.path as osp\n",
    "import itertools\n",
    "import astropy.io.fits as fits\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from pprint import pprint\n",
    "plt.style.use('seaborn-ticks')\n",
    "import warnings\n",
    "from sklearn.utils import resample\n",
    "\n",
    "### Metrics\n",
    "# from sources.ml_f1 import*\n",
    "from sources.ml_precision import*\n",
    "# from sources.recall_review import*\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score as accuracy\n",
    "from sklearn.metrics import f1_score as f1\n",
    "from sklearn.metrics import recall_score as recall\n",
    "from sklearn.metrics import precision_score as precision\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.spatial import distance\n",
    "\n",
    "# ML models  \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import xgboost\n",
    "from xgboost import plot_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034ddf3a-58f2-4408-a270-6568f3d221d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "catalogue = pd.read_csv('old-test-train/raw_data_original.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34bc1c79-8623-4f47-bb65-2d85b782e46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = catalogue.drop(['class_labels'], axis = 1)\n",
    "labels = catalogue['class_labels']\n",
    "\n",
    "# encoding target class\n",
    "y, clas = pd.factorize(labels) #getting the class 0 = agn, 1 =notagn, 2 = no class\n",
    "y_target = pd.DataFrame(y, columns = ['class_labels'])\n",
    "\n",
    "data = pd.concat([X, y_target], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3738b7-3961-4ce5-b4a0-efb74a574a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data\n",
    "   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aada0464-b16f-4c31-b0b3-f7c69fb6e5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate classes\n",
    "class_counts = data['class_labels'].value_counts()\n",
    "majority_class = class_counts.idxmax()\n",
    "minority_class = class_counts.idxmin()\n",
    "\n",
    "df_majority = data[data['class_labels'] == majority_class]\n",
    "df_minority = data[data['class_labels'] == minority_class]\n",
    "\n",
    "# Downsample majority class and keep the removed samples\n",
    "df_majority_downsampled = resample(df_majority,\n",
    "                                   replace=False,    # sample without replacement\n",
    "                                   n_samples=len(df_minority),  # to match minority class\n",
    "                                   random_state=42)  # reproducible results\n",
    "\n",
    "# Get the removed samples by finding the difference between original and downsampled\n",
    "removed_samples = df_majority[~df_majority.index.isin(df_majority_downsampled.index)]\n",
    "\n",
    "# Combine minority class with downsampled majority class\n",
    "df_balanced = pd.concat([df_majority_downsampled, df_minority])\n",
    "\n",
    "# Split back into X and y\n",
    "X_balanced = df_balanced.drop('class_labels', axis=1)\n",
    "y_balanced = df_balanced['class_labels']\n",
    "X_removed = removed_samples.drop('class_labels', axis=1)\n",
    "y_removed = removed_samples['class_labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e964abd-9f62-4292-b82d-8f3b7477bbc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The Original Classes: \", class_counts)\n",
    "print(\"The Balanced Classes: \", df_balanced['class_labels'].value_counts())\n",
    "print('Removed sample: ', removed_samples['class_labels'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40499ff-5991-4a61-b117-5a76c9c4ea6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb4329f-5b88-4530-b8dd-210c6abdb7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "removed_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746abac1-3640-4f01-aada-43fae34f1838",
   "metadata": {},
   "outputs": [],
   "source": [
    "## random forest (RF)\n",
    "# The Random Hyper parameter Grid\n",
    "\n",
    "# number of trees in the forest\n",
    "n_estimators = [50, 100, 150]\n",
    "\n",
    "# Number of feature to consider at every split\n",
    "max_features = [2, 3]\n",
    "\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [5, 10]\n",
    "\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5]\n",
    "\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 3]\n",
    "\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "\n",
    "\n",
    "# Create the random grid\n",
    "rf_par = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth}\n",
    "               # 'min_samples_split': min_samples_split,\n",
    "               # 'min_samples_leaf': min_samples_leaf,\n",
    "               # 'bootstrap': bootstrap}\n",
    "\n",
    "\n",
    "\n",
    "rf_model= RandomForestClassifier(random_state=1)\n",
    "rf_par = dict(n_estimators=n_estimators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81119710-d3b1-44e4-b8a2-d9664232f2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN model\n",
    "knn_model = KNeighborsClassifier()\n",
    "\n",
    "## KNN parameters\n",
    "knn_par = {'n_neighbors' : [5, 10, 15], 'p':[1, 2], 'weights' : ['uniform', 'distance'] }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73fe959-bfc6-4e62-bf7b-107fb11ea6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up models and Parameters for a \"for loop\"  \n",
    "\n",
    "models = [[knn_model, 'knn'], [rf_model, 'rf']]\n",
    "\n",
    "parameters = [ knn_par, rf_par]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a130c1c5-8d73-4b13-8af7-85e16aa8ffd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_dicts = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef8da38-7e5e-4a2c-a3bf-46f968c3129b",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [['qir', 'class_star', 'log(S8/S45)','log(S58/S36)', 'Mstar', 'log(S45/S36)']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca7ec66-9adb-44d2-8875-e03945b081a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = [0.8, 0.6, 0.4, 0.2]\n",
    "tr_sizes = [0.2, 0.4, 0.6, 0.8]\n",
    "\n",
    "X_test = X_removed\n",
    "y_test = y_removed\n",
    "\n",
    "# Loop through different ML models coupled with thier hyper paramter (use the same splits for all features)\n",
    "for m, par in zip(models, parameters):\n",
    "    key0 = str(m[1])\n",
    "    print(key0)\n",
    "    ml_dicts[key0] = {} # defining The main subkeys, which are the machine learning models\n",
    "        \n",
    "    for s, tr in zip(splits, tr_sizes):\n",
    "        X_train, X_vald, y_train, y_vald = train_test_split(X_balanced, y_balanced, test_size= s, random_state=1, stratify = y_balanced, shuffle = True)\n",
    "        \n",
    "        i = 1\n",
    "        for f in features:\n",
    "            xtr =  X_train[f]\n",
    "            xva =  X_vald[f]\n",
    "            xte =  X_test[f]\n",
    "            \n",
    "            # results = get_f1_ml (m[0], par, xtr, y_train, xva, y_vald, xte, y_test) # to get the f1 for the ml model\n",
    "            results = get_precision_ml (m[0], par, xtr, y_train, xva, y_vald, xte, y_test, split=s) # to get the f1 for the ml model\n",
    "            # results = get_recall_ml (m[0], par, xtr, y_train, xva, y_vald, xte, y_test) # to get the f1 for the ml model\n",
    "            \n",
    "\n",
    "            key = str(tr)+\", F\"+str((i)) # Create keys for the each feature set in order to reference results\n",
    "            ml_dicts[key0][key] = {}\n",
    "\n",
    "            ml_dicts[key0][key]['tot_f1_vald'] = results[0]\n",
    "            ml_dicts[key0][key]['tot_f1_test'] = results[1]\n",
    "            ml_dicts[key0][key]['jack_train'] = results[2]\n",
    "            ml_dicts[key0][key]['jack_vald'] = results[3]\n",
    "            ml_dicts[key0][key]['jack_test'] = results[4]\n",
    "            i += 1\n",
    "            \n",
    "            \n",
    "import json\n",
    "with open('knn_rf_comparison_original.txt', 'w') as file:\n",
    "    file.write(json.dumps(ml_dicts)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b866db74-c9d8-414a-a5e9-3e7ff5775626",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_all = []\n",
    "for m, d in zip (models, ml_dicts.keys()):\n",
    "    f1_arr_vald = []\n",
    "    f1_arr_test = []\n",
    "    sd_vald_arr = []\n",
    "    sd_arr = [] \n",
    "    \n",
    "    # print(ml_dicts[d])\n",
    "    for key in ml_dicts[d].keys():\n",
    "        f1_arr_vald.append(ml_dicts[d][key][ 'tot_f1_vald' ]) # append total valdation f1 score to an array\n",
    "        f1_arr_test.append(ml_dicts[d][key][ 'tot_f1_test' ]) # append total test f1 score to an array\n",
    "        \n",
    "        sd_train = jack_SD(np.zeros( len(ml_dicts[d][key][ 'jack_train' ]) ), ml_dicts[d][key][ 'jack_train' ])[0]\n",
    "        sd_vald = jack_SD(np.zeros( len(ml_dicts[d][key][ 'jack_vald' ]) ), ml_dicts[d][key][ 'jack_vald' ])[0]\n",
    "        sd_test = jack_SD(np.zeros( len(ml_dicts[d][key][ 'jack_test' ]) ), ml_dicts[d][key][ 'jack_test' ])[0]\n",
    "        \n",
    "        sd_v = np.sqrt( np.array((sd_train**2)) + np.array((sd_vald**2)))\n",
    "        sd = np.sqrt( np.array((sd_train**2)) + np.array((sd_test**2)))\n",
    "       \n",
    "        sd_vald_arr.append(sd_v)\n",
    "        sd_arr.append(sd)\n",
    "        # append the SD to the sd_arr\n",
    "    arr_all.append([ list(ml_dicts[d].keys()), f1_arr_vald, f1_arr_test, sd_vald_arr, sd_arr])    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef92c92-1c10-4296-84b2-034fee6fa2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# Your data\n",
    "data = arr_all\n",
    "# Define headers\n",
    "headers = [\n",
    "    \"Train Fraction\",\n",
    "    \"KNN Validation Score\", \"KNN Test Score\", \"KNN Val Error\", \"KNN Test Error\",\n",
    "    \"RF Validation Score\", \"RF Test Score\", \"RF Val Error\", \"RF Test Error\"\n",
    "]\n",
    "\n",
    "# Extract data\n",
    "train_fractions = [0.2, 0.4, 0.6, 0.8]\n",
    "knn_val_scores = data[0][1]\n",
    "knn_test_scores = data[0][2]\n",
    "knn_val_errors = data[0][3]\n",
    "knn_test_errors = data[0][4]\n",
    "rf_val_scores = data[1][1]\n",
    "rf_test_scores = data[1][2]\n",
    "rf_val_errors = data[1][3]\n",
    "rf_test_errors = data[1][4]\n",
    "\n",
    "# Write to CSV\n",
    "with open('normalised/recall_balanced_13_july_2.csv', 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(headers)  # Write header row\n",
    "    for i, frac in enumerate(train_fractions):\n",
    "        row = [\n",
    "            frac,\n",
    "            knn_val_scores[i], knn_test_scores[i], knn_val_errors[i], knn_test_errors[i],\n",
    "            rf_val_scores[i], rf_test_scores[i], rf_val_errors[i], rf_test_errors[i]\n",
    "        ]\n",
    "        writer.writerow(row)\n",
    "\n",
    "print(\"CSV file 'model_comparison.csv' has been created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96dda67a-7648-4692-979f-bcccf813ee97",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d68789-55f9-4f8c-9ddb-127724e14ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = arr_all\n",
    "# Extract x-axis values (fraction of train size)\n",
    "x = [0.2, 0.4, 0.6, 0.8]\n",
    "\n",
    "# Prepare data for KNN and Random Forest\n",
    "knn_val_scores = data[0][1]\n",
    "knn_test_scores = data[0][2]\n",
    "knn_val_errors = data[0][3]\n",
    "knn_test_errors = data[0][4]\n",
    "\n",
    "rf_val_scores = data[1][1]\n",
    "rf_test_scores = data[1][2]\n",
    "rf_val_errors = data[1][3]\n",
    "rf_test_errors = data[1][4]\n",
    "\n",
    "# Plot settings\n",
    "# plt.figure(figsize=(10, 6))\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.xlabel('Fraction of Train Size', fontsize=18)\n",
    "plt.ylabel('F1 Score', fontsize=18)\n",
    "# plt.title('Comparison of KNN and Random Forest')\n",
    "plt.xticks(x, fontsize=18)\n",
    "plt.yticks(fontsize=18)\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "# Plot KNN\n",
    "plt.errorbar(x, knn_val_scores, yerr=knn_val_errors, fmt='s', label='kNN', capsize=5, color='blue')\n",
    "# plt.errorbar(x, knn_test_scores, yerr=knn_test_errors, fmt='--o', label='KNN (Test)', capsize=5, color='lightblue')\n",
    "\n",
    "# Plot Random Forest\n",
    "plt.errorbar(x, rf_val_scores, yerr=rf_val_errors, fmt='o', label='RF', capsize=5, color='green')\n",
    "# plt.errorbar(x, rf_test_scores, yerr=rf_test_errors, fmt='o', label='Random Forest (Test)', capsize=5, color='lightgreen')\n",
    "\n",
    "# Add grid and adjust layout\n",
    "plt.ylim(0.8, 1.)\n",
    "plt.grid(True, linestyle='--', alpha=0.4)\n",
    "plt.tight_layout()\n",
    "plt.legend(loc=\"lower right\", fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326281ad-146d-4d91-9c45-eb6376160be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# Example data (replace with your actual data)\n",
    "y_true = y_removed  # Only positive class (1)\n",
    "y_pred = pd.read_csv('normalised/yPred_SFGs.csv')  # Contains both classes\n",
    "y_pred_knn_08 = pd.read_csv('normalised/KNeighborsClassifier()0.2_yPred_SFGs.csv')  # Contains both classes\n",
    "y_pred_knn_02 = pd.read_csv('normalised/KNeighborsClassifier()0.8_yPred_SFGs.csv')  # Contains both classes\n",
    "\n",
    "# Calculate metrics\n",
    "precision = precision_score(y_true, y_pred)\n",
    "recall = recall_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "print(f\"Precision: {precision:.4f}\")  # TP / (TP + FP)\n",
    "print(f\"Recall: {recall:.4f}\")       # TP / (TP + FN)\n",
    "print(f\"F1 Score: {f1:.4f}\")         # 2 * (precision * recall) / (precision + recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b99aeae-3347-4b9c-89e1-29021461e1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred.value_counts()\n",
    "print(\"02 Train Size: \",y_pred_knn_02.value_counts())\n",
    "print(\"08 Train Size: \",y_pred_knn_08.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53354dda-5d4a-42ea-8a54-6796d373d616",
   "metadata": {},
   "outputs": [],
   "source": [
    "def man_confusion_matrix(cm, classes, \n",
    "                        name = '',\n",
    "                        normalize=False,\n",
    "                        title='Confusion Matrix',\n",
    "                        cmap=plt.cm.Greens):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title, fontsize=26)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, fontsize=20,rotation=45)\n",
    "    plt.yticks(tick_marks, classes, fontsize=20)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "    \n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\",\n",
    "                 fontsize=28)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True Label', fontsize=20)\n",
    "    plt.xlabel('Predicted Label', fontsize=20)\n",
    "    plt.savefig(name)\n",
    "    plt.show()# Feature importance for the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc11925d-9422-486d-a46a-c41ae364cba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute confusion matrix for kNN classifier\n",
    "cm_test = metrics.confusion_matrix( y_true, y_pred)\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plt.figure(figsize=(11,11))\n",
    "man_confusion_matrix(cm_test,classes=['AGN','SFG'], name = 'cm_sfgs', normalize=True,\n",
    "                      title='Normalized confusion matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28fe0e86-4f1f-4f3a-ac6f-0c1412921847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute confusion matrix for kNN classifier\n",
    "cm_test = metrics.confusion_matrix( y_true, y_pred_knn_02)\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plt.figure(figsize=(11,11))\n",
    "man_confusion_matrix(cm_test,classes=['AGN','SFG'], name = 'cm_sfgs_02', normalize=True,\n",
    "                      title='02 Train Size confusion matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d354a68-6e07-45a5-8c67-293ce75eb6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute confusion matrix for kNN classifier\n",
    "cm_test = metrics.confusion_matrix( y_true, y_pred_knn_08)\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plt.figure(figsize=(11,11))\n",
    "man_confusion_matrix(cm_test,classes=['AGN','SFG'], name = 'cm_sfgs_08', normalize=True,\n",
    "                      title='08 Train Size confusion matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b5ee36-ee43-407a-9d16-4ad88ffa6fb3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ASTRO-PY3",
   "language": "python",
   "name": "astro-py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
